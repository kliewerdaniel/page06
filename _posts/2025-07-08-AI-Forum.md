---
layout: post
title: AI Forum
description:  Build an AI-Powered Q&A Forum with Server Actions, Streaming, and Agentic Memory in Next.js 14+
date:   2025-07-08 02:42:44 -0500
---

**Build an AI-Powered Q&A Forum with Server Actions, Streaming, and Agentic Memory in Next.js 14+**

---

## **🧭 Introduction**

  

> A long-form overview of what this project is, what problems it solves, how it integrates current Next.js 14+ architecture with LLM workflows, and why it’s more than a chatbot — it’s an interactive thinking tool powered by structure, memory, and autonomy.

---

## **1.** 

## **Project Setup**

  

> Set up your Next.js 14 App Router project using modern defaults. Configure local storage (JSON, SQLite, or DB) for thread data, install optional memory tools (like ChromaDB), and prepare your environment to run local or cloud-hosted LLMs (e.g. via Ollama or OpenAI).

---

## **2.** 

## **Architectural Overview**

  

> A conceptual overview of how conversation threads are stored, retrieved, and rendered using Server Components. Learn the difference between structural memory (thread history) and semantic memory (embedding retrieval), and see how this system becomes a modular knowledge engine.

---

## **3.** 

## **Thread Data Structure and Persistence**

  

> Define how threads and messages are stored. Understand the object shape ({ id, parentId, role, content, timestamp }) and how to build a flexible backend using file-based, SQLite, or cloud DB storage.

---

## **4.** 

## **Rendering with React Server Components**

  

> Use React Server Components (RSC) to render threaded Q&A UI. Render messages on the server at request time. Learn how Suspense and Loading UI enhance experience without extra client-side logic.

---

## **5.** 

## **Forms with Server Actions**

  

> Handle form submissions using Next.js 14’s new Server Actions instead of classic API endpoints. Learn how to process new replies securely on the server and re-render updated pages — without any client-side JS needed.

---

## **6.** 

## **AI Integration with Local or Hosted Models**

  

> Format thread history and recent replies into an LLM-friendly prompt. Use OpenAI or local models (e.g., Qwen2 via Ollama) to generate coherent assistant replies. Optionally, stream responses using the Vercel AI SDK.

---

## **7.** 

## **Adding Semantic Memory with Vector Search**

  

> Go beyond static threads: embed past messages into a vector store like ChromaDB, and query them to dynamically inject relevant memory into prompts. Mimics “recall” and allows cross-thread intelligence.

---

## **8.** 

## **Building the Threaded UI**

  

> Create nested React components to render multi-level conversations. Display replies under parents with indentation or UI styling. Build interactive reply forms using RSC + progressive enhancement.

---

## **9.** 

## **Bonus Features and UX Enhancements**

  

> Add real-time summarization agents, markdown formatting, user authentication, a home page with recent threads, streaming assistant responses, and more. These features help take your demo from prototype to product-ready.

---

## **10.** 

## **Agentic Graph Expansion (Optional)**

  

> Introduce modular agents as nodes in a thought graph. Implement orchestrated flows like User → Generator → Reflector → Summarizer. Visualize thread graphs with ReactFlow. This section bridges frontend UX with backend cognition.

---

## **11.** 

## **Deployment & Local-First LLMs**

  

> Deploy your app using Vercel’s Edge Network or run it entirely locally using Ollama, ChromaDB, and SQLite. This section includes notes on performance, quantized model selection, and running without any cloud dependencies.

---

## **12.** 

## **Future Directions**

  

> Explore journaling integration, emotion-based memory, biometric feedback, persona evolution, and building tools that adapt to how you think. These ideas are inspired by agentic knowledge graph research and provide inspiration for building personal AIs.

---

## **🧾 Appendix: Code Snippets & APIs**

  

> Includes reusable code snippets for form actions, memory queries, thread rendering, agent orchestration, and LLM prompt structures. Think of this as your development cheatsheet.

---

# **🧠 Introduction**

  

In 2023 and 2024, the AI development world revolved around chatbots and content generation tools. They were impressive — they could explain anything, draft marketing copy, or simulate a character. But if you’ve used them for real work, you’ve probably hit a wall:

- They forget what you said two turns ago.
    
- They generate helpful text, but lack continuity or memory.
    
- They respond, but they don’t think with you.
    

  

This guide is for builders who want to go beyond chat — to create **AI-powered thinking systems** that evolve, remember, and reflect.

---

We’ll start by building something that feels simple: a threaded Q&A forum. But the twist is that each reply comes not from a human, but from a powerful language model — one that can maintain coherent multi-turn conversations by using **stored context**, **semantic memory**, and even **graph-based orchestration of agent tasks**.

  

You won’t just build a “chat” interface. You’ll create:

- A **threaded knowledge forum** where every conversation has depth.
    
- A **streaming AI assistant** that answers in real time using React Server Components.
    
- A **modular architecture** where LLMs become agents that you orchestrate.
    
- A system that can be deployed **on the Edge** or run entirely **on your machine**, powered by local-first tools like **Ollama** and **ChromaDB**.
    
- A seed for **personal thought infrastructure** — where AI becomes an extension of your mind, not just a productivity gimmick.
    

---

By combining the **App Router in Next.js 14+**, **Server Actions**, **React Suspense**, **semantic vector search**, and **agentic knowledge graph design**, this guide shows how modern web development and local AI tooling can finally meet.

  

You’ll come away with a production-ready AI app — but also with a new way of thinking about LLMs:

  

> **Not as oracles, but as structured systems.**

> **Not as assistants, but as collaborators.**

> **Not as tools for writing, but for reasoning.**

  

Let’s begin.

---


# **Chapter 1: Project Setup**

  

In this chapter, you’ll scaffold a brand-new project using **Next.js 14 with the App Router**, configure a minimal backend for storing conversation threads, and set the stage for integrating local or cloud-based LLMs. You’ll also prepare your dev environment to support optional features like memory embeddings and graph orchestration.

---

## **✅ 1.1 Create Your Next.js 14 App**

  

We’ll use the latest **App Router** features, which are fully stable in Next.js 14.

  

In your terminal:

```
npx create-next-app@latest ai-thread-forum
```

**Use the following options when prompted:**

- ✅ TypeScript – **Yes**
    
- ✅ App Router – **Yes**
    
- ✅ Tailwind CSS – Optional, but recommended for quick styling
    
- ✅ ESLint – Yes
    
- ✅ src/ directory – Your choice (this guide assumes **not using it** for simplicity)
    
- ✅ Import alias – Optional
    

  

Once installed:

```
cd ai-thread-forum
npm run dev
```

Your app should now be running at: http://localhost:3000.

---

## **🗃 1.2 Setup a Minimal Backend for Thread Storage**

  

We’ll start with a simple **JSON file-based storage system**, which can later be replaced with SQLite, Supabase, or any DB.

  

Create a new directory:

```
/data/threads/
```

Inside this folder, each thread will be saved as a file:

  

**Example path:**

```
/data/threads/abc123.json
```

**Example thread file:**

```
{
  "id": "abc123",
  "title": "How does SSR work?",
  "messages": [
    {
      "id": "1",
      "role": "user",
      "content": "What is server-side rendering?",
      "parentId": null,
      "timestamp": 1720000000000
    },
    {
      "id": "2",
      "role": "assistant",
      "content": "SSR means rendering the HTML on the server...",
      "parentId": "1",
      "timestamp": 1720000001000
    }
  ]
}
```

We’ll write utility functions in /lib/threadStore.ts to load and save threads.

---

## **💾 1.3 Add Required Dependencies**

  

Run the following:

```
npm install uuid
```

This helps us generate unique message/thread IDs.

  

Optional (for vector memory):

```
npm install chromadb sentence-transformers
```

Optional (if using local LLMs via Ollama):

```
# install ollama CLI: https://ollama.com
# then run this:
ollama run mistral  # or qwen2, llama3, etc.
```

---

## **⚙ 1.4 Create Utility Functions for Storage**

  

In lib/threadStore.ts:

```
import fs from 'fs/promises';
import path from 'path';

const THREAD_DIR = path.join(process.cwd(), 'data/threads');

export async function getThread(id: string) {
  const filePath = path.join(THREAD_DIR, `${id}.json`);
  const raw = await fs.readFile(filePath, 'utf8');
  return JSON.parse(raw);
}

export async function saveThread(id: string, data: any) {
  const filePath = path.join(THREAD_DIR, `${id}.json`);
  await fs.writeFile(filePath, JSON.stringify(data, null, 2), 'utf8');
}
```

We’ll use this in Server Components and Server Actions to fetch and persist threads.

---

## **🧠 1.5 LLM Backend Setup**

  

### **Option A:** 

### **OpenAI**

  

If you have access to OpenAI:

```
npm install openai
```

Set up an .env.local file:

```
OPENAI_API_KEY=sk-...
```

Create a helper function in /lib/llm.ts:

```
import { OpenAI } from 'openai';

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

export async function callLLM(messages) {
  const res = await openai.chat.completions.create({
    model: 'gpt-4',
    messages,
  });
  return res.choices[0].message.content;
}
```

---

### **Option B:** 

### **Local Models with Ollama**

  

Start Ollama:

```
ollama run mistral
```

Then create a call function:

```
export async function callLLM(messages) {
  const prompt = messages.map(m => `${m.role}: ${m.content}`).join('\n');
  const res = await fetch('http://localhost:11434/api/generate', {
    method: 'POST',
    body: JSON.stringify({
      model: 'mistral',
      prompt,
      stream: false
    }),
    headers: { 'Content-Type': 'application/json' }
  });
  const data = await res.json();
  return data.response;
}
```

---

✅ With this, your project is fully scaffolded with:

- Next.js 14 (App Router)
    
- A simple persistent storage system
    
- Hooks for local or remote LLMs
    
- Optional memory system for future steps
    

---

🟢 You’re ready to build the architecture that ties all these parts together.

---


# **Chapter 2: Architectural Overview**

  

In this chapter, we’ll outline the structure of the application from three angles:

- The **threading system**: how Q&A messages are linked and displayed.
    
- The **AI inference pipeline**: how user messages turn into model prompts.
    
- The **agentic memory model**: how semantic memory (embeddings) and structure (threads) co-exist.
    

  

You’ll walk away knowing how all the parts work together — like a mental map for the code you’re about to write.

---

## **🏗️ 2.1 System Architecture at a Glance**

```
[User ↩️ UI Form] 
    ↓
[Server Action: submitReply()]
    ↓
[Thread Store (File/DB)]
    ↓
[LLM (OpenAI or Ollama)]
    ↓
[Agent Response]
    ↓
[Update Thread]
    ↓
[RSC Page Render]
```

**Frontend** is a mix of **React Server Components (RSC)** and progressive HTML forms.

**Backend** is structured around Server Actions and a minimal storage layer.

**LLM** is abstracted behind callLLM(), and can be remote or local.

---

## **🔄 2.2 Thread as a Tree**

  

Each conversation thread is stored as a list of message objects:

```
{
  id: '2',
  role: 'assistant',
  content: 'Sure, here’s how SSR works...',
  parentId: '1',
  timestamp: 1720001000000
}
```

With parentId, we can build a **thread tree** like:

```
Q1: "What is SSR?"
  ↳ A1: "Server-side rendering means..."
    ↳ Q1a: "How is that different from CSR?"
      ↳ A1a: "Client-side rendering defers until load..."
```

We’ll build a recursive component that walks this tree and renders the conversation in context.

---

## **🧠 2.3 The Prompt Assembly Chain**

  

Every time a user submits a new message, we’ll do the following:

1. **Fetch the full thread** from storage.
    
2. **Flatten** the tree into a list of previous messages in chronological order (from root to now).
    
3. Format that into a **prompt array**:
    

```
[
  { role: 'system', content: 'You are a helpful assistant.' },
  { role: 'user', content: 'What is SSR?' },
  { role: 'assistant', content: 'SSR means rendering HTML on the server...' },
  { role: 'user', content: 'How is that different from CSR?' }
]
```

4. (Optional) Prepend **semantic memory context**, e.g. relevant past threads or replies.
    
5. **Send** to LLM and receive the assistant’s reply.
    
6. **Append** the new assistant message to the thread and save.
    

---

## **🕸 2.4 Memory: Structural vs Semantic**

  

Inspired by the Agentic Knowledge Graph concept, we treat memory as **two kinds**:

  

### **🧩 Structural Memory**

- Threaded replies and relationships.
    
- Parent-child chains (who responded to what).
    
- Used to maintain _conversation state_.
    

  

### **🔍 Semantic Memory (Optional)**

- Embeddings of past messages stored in ChromaDB (or similar).
    
- Queried when generating a new assistant reply to inject context from _similar topics_.
    
- Useful for surfacing related questions across threads, not just within.
    

  

You’ll implement semantic memory later in Chapter 7 — but design for it from the start by leaving a placeholder in the LLM call pipeline.

---

## **🔀 2.5 Orchestrating Thought as a Graph (Optional Future Step)**

  

If you want to scale beyond a single AI assistant per thread, you can model each agent (e.g., summarizer, critic, reflector) as a node in a **task graph**:

```
[User Question]
   ↓
[Generator Agent] → writes first draft
   ↓
[Critic Agent] → evaluates tone or clarity
   ↓
[Summarizer Agent] → generates TL;DR
```

This graph-based design (from the Agentic Knowledge Graph ebook) enables powerful workflows like:

- Generating summaries from long threads
    
- Routing emotional messages to a reflective agent
    
- Self-evolving threads that rewrite themselves
    

  

We’ll revisit this in **Chapter 10**, but you can keep it in mind as you build the basics.

---

## **✅ Summary**

  

By the end of this chapter, you should understand:

- How threads are stored, related, and rendered.
    
- How messages are chained into prompts for the LLM.
    
- How structural and semantic memory fit together.
    
- How your future system could evolve into a modular graph of thinking agents.
    

---

> You’re now ready to implement the storage and data model that powers these threads.


---


# **Chapter 3: Thread Data Structure and Persistence**

  

Now that you understand the architecture, it’s time to implement the foundational **thread model** — how messages are structured, stored, and retrieved. In this chapter, you’ll define a minimal, extensible format for Q&A conversations, add utility functions to read and write them, and prepare for branching replies using parent-child relationships.

---

## **🧱 3.1 Message Object Format**

  

Each message in a thread is an object with the following properties:

```
type Message = {
  id: string;             // unique message ID
  parentId: string | null; // reply to which message (null = top-level question)
  role: 'user' | 'assistant';
  content: string;
  timestamp: number;
};
```

All messages are stored together in a thread object:

```
type Thread = {
  id: string;
  title: string;
  messages: Message[];
};
```

This structure supports:

- **Threaded replies** (by following parentId)
    
- **Multi-turn conversations** (via role)
    
- **Nesting + reply chains** (arbitrary depth)
    
- **Easy flat storage in JSON or a database**
    

---

## **📂 3.2 File-Based Thread Store (MVP)**

  

To start, we’ll store each thread in a file:

```
/data/threads/[id].json
```

In /lib/threadStore.ts, write:

```
import fs from 'fs/promises';
import path from 'path';
import { v4 as uuidv4 } from 'uuid';

const THREAD_DIR = path.join(process.cwd(), 'data/threads');

export async function getThread(id: string) {
  const filePath = path.join(THREAD_DIR, `${id}.json`);
  const raw = await fs.readFile(filePath, 'utf8');
  return JSON.parse(raw);
}

export async function saveThread(thread: any) {
  const filePath = path.join(THREAD_DIR, `${thread.id}.json`);
  await fs.writeFile(filePath, JSON.stringify(thread, null, 2));
}

export async function createThread(title: string, question: string) {
  const id = uuidv4();
  const messageId = uuidv4();
  const thread = {
    id,
    title,
    messages: [
      {
        id: messageId,
        role: 'user',
        content: question,
        parentId: null,
        timestamp: Date.now(),
      }
    ]
  };
  await saveThread(thread);
  return thread;
}
```

This gives us:

- getThread(id) — read a thread from file
    
- saveThread(thread) — write updated thread
    
- createThread(title, question) — create new with initial message
    

---

## **🧬 3.3 In-Memory Caching (Optional for Dev Speed)**

  

If you’re working on a local project and want fast reloads, add a basic in-memory cache:

```
const cache = new Map();

export async function getThread(id: string) {
  if (cache.has(id)) return cache.get(id);
  const filePath = path.join(THREAD_DIR, `${id}.json`);
  const raw = await fs.readFile(filePath, 'utf8');
  const thread = JSON.parse(raw);
  cache.set(id, thread);
  return thread;
}
```

You’ll want to clear or expire the cache in production.

---

## **🔁 3.4 Adding a New Message to a Thread**

  

Define a helper function to push a new user or assistant message:

```
export async function addMessageToThread(threadId: string, role: 'user' | 'assistant', content: string, parentId: string | null) {
  const thread = await getThread(threadId);
  const message = {
    id: uuidv4(),
    role,
    content,
    parentId,
    timestamp: Date.now(),
  };
  thread.messages.push(message);
  await saveThread(thread);
  return message;
}
```

This allows:

- Top-level messages (when parentId = null)
    
- Nested replies (any depth)
    
- Time-ordered histories (we’ll sort later when rendering)
    

---

## **🌳 3.5 Flattening Thread for Prompt Input**

  

Before calling the LLM, you’ll need to **reconstruct the conversation path** for context.

```
export function buildPromptFromThread(messages: Message[], currentId: string) {
  const chain: Message[] = [];

  let current = messages.find(m => m.id === currentId);
  while (current) {
    chain.unshift(current);
    current = current.parentId ? messages.find(m => m.id === current.parentId) : null;
  }

  return [
    { role: 'system', content: 'You are a helpful assistant.' },
    ...chain.map(m => ({ role: m.role, content: m.content }))
  ];
}
```

This gives you the exact message trail that led to the current input — perfect for LLM context windows.

---

## **🧪 3.6 Test Your Setup**

  

In a temporary app/test/page.tsx, you can try:

```
import { createThread, getThread } from '@/lib/threadStore';

export default async function TestPage() {
  const thread = await createThread('Test SSR', 'How does server-side rendering work?');
  const reloaded = await getThread(thread.id);

  return (
    <div>
      <h1>Thread: {reloaded.title}</h1>
      {reloaded.messages.map(msg => (
        <pre key={msg.id}>{msg.role}: {msg.content}</pre>
      ))}
    </div>
  );
}
```

Then visit /test to confirm data is loading.

---

## **✅ Summary**

  

You now have:

- A flexible message + thread format
    
- Utility functions to create, retrieve, and update threads
    
- A prompt builder that reassembles message chains
    
- A ready-to-use backend for Server Actions
    

---

> Up next: rendering the full thread using **React Server Components** — with nested replies, streaming, and forms.

  
---

# **Chapter 4: Rendering with React Server Components**

  

In this chapter, you’ll render your threaded Q&A conversations entirely **server-side** using **React Server Components (RSC)** — a modern Next.js 14 feature that optimizes performance and SEO without writing manual SSR logic.

  

We’ll also structure the message UI to handle **nested replies** and prepare forms for replying to specific messages. This creates the foundation of your interactive forum interface.

---

## **🧭 4.1 How React Server Components Work (Recap)**

  

With the **App Router** in Next.js 14:

- Every file in /app/ is a **server component by default**.
    
- This means no more getServerSideProps, getStaticProps, or useEffect just to load data.
    
- Your data fetching is done at the **top level of the component**, on the server, at request time.
    
- The result? You render complete HTML that gets hydrated on the client **only where needed**.
    

---

## **📁 4.2 Create the Dynamic Thread Page**

  

In app/thread/[id]/page.tsx:

```
import { getThread } from '@/lib/threadStore';
import { ThreadMessage } from './ThreadMessage';
import { NewReplyForm } from './NewReplyForm';

export default async function ThreadPage({ params }: { params: { id: string } }) {
  const thread = await getThread(params.id);

  return (
    <div className="max-w-3xl mx-auto p-4">
      <h1 className="text-2xl font-bold mb-4">{thread.title}</h1>

      <div className="space-y-4">
        {thread.messages
          .filter(msg => msg.parentId === null)
          .map(msg => (
            <ThreadMessage
              key={msg.id}
              message={msg}
              allMessages={thread.messages}
              threadId={thread.id}
            />
          ))}
      </div>

      <div className="mt-8">
        <h2 className="font-semibold mb-2">Add New Question</h2>
        <NewReplyForm threadId={thread.id} parentId={null} />
      </div>
    </div>
  );
}
```

- This renders top-level messages (no parentId)
    
- Then recursively renders their children (via ThreadMessage)
    
- Each message includes a form to reply
    

---

## **💬 4.3 ThreadMessage Component (Recursive)**

  

Create app/thread/[id]/ThreadMessage.tsx:

```
import { Message } from '@/types';
import { NewReplyForm } from './NewReplyForm';

type Props = {
  message: Message;
  allMessages: Message[];
  threadId: string;
  depth?: number;
};

export function ThreadMessage({ message, allMessages, threadId, depth = 0 }: Props) {
  const replies = allMessages.filter(m => m.parentId === message.id);

  return (
    <div className={`ml-${depth * 4} border-l pl-4 py-2`}>
      <div className="mb-2">
        <span className="text-sm font-semibold">{message.role}</span>: {message.content}
      </div>

      <NewReplyForm threadId={threadId} parentId={message.id} />

      {replies.map(reply => (
        <ThreadMessage
          key={reply.id}
          message={reply}
          allMessages={allMessages}
          threadId={threadId}
          depth={depth + 1}
        />
      ))}
    </div>
  );
}
```

- This nests replies visually with left indentation (ml-x)
    
- Each message can be replied to individually
    
- It recursively renders all descendants
    

  

✅ You now have full **threaded rendering** with depth-based layout.

---

## **✍️ 4.4 Build the NewReplyForm Component**

  

In app/thread/[id]/NewReplyForm.tsx:

```
'use client';

import { useRef } from 'react';

type Props = {
  threadId: string;
  parentId: string | null;
};

export function NewReplyForm({ threadId, parentId }: Props) {
  const formRef = useRef<HTMLFormElement>(null);

  return (
    <form
      action={`/api/submit?threadId=${threadId}&parentId=${parentId || ''}`}
      method="POST"
      ref={formRef}
      className="space-y-2 mt-2"
    >
      <textarea name="content" rows={2} required className="w-full border p-2 rounded" />
      <div>
        <button type="submit" className="text-sm text-blue-600 hover:underline">
          Reply
        </button>
      </div>
    </form>
  );
}
```

You’ll later convert this to a **Server Action** in Chapter 5, but for now it uses a classic POST for progressive enhancement.

---

## **🧪 4.5 View a Thread in the Browser**

  

Visit:

```
http://localhost:3000/thread/[some-id]
```

You should see:

- The thread title
    
- The top-level user question
    
- The AI reply indented
    
- An input form below each message
    

---

## **🧼 4.6 (Optional) Message Styling**

  

Improve the readability by distinguishing user/assistant roles:

```
<div className={`mb-2 p-2 rounded ${message.role === 'user' ? 'bg-blue-100' : 'bg-gray-100'}`}>
  <span className="text-sm font-semibold">{message.role}</span>: {message.content}
</div>
```

---

## **✅ Summary**

  

You now have:

- A fully **server-rendered thread page** using Next.js 14’s App Router
    
- A recursive component structure for **nested message threads**
    
- A basic form structure that supports **multi-turn replies**
    

  

You’re rendering everything on the server — no client-side fetching required — and your app is ready to handle message submission with modern Server Actions.

---

> In the next chapter, we’ll wire up those reply forms to a **Server Action**, and let the AI generate real responses.

  

---


# **Chapter 5: Forms with Server Actions**

  

In this chapter, you’ll implement **Server Actions**, one of the most powerful features introduced in **Next.js 14**. Server Actions let you handle form submissions directly inside your React component file — no need for API routes or client-side mutations.

  

This not only simplifies your code, but also improves performance and security. You’ll wire up your reply form so that when a user submits a message, it:

1. Updates the thread with their message,
    
2. Calls the LLM (local or OpenAI),
    
3. Appends the AI’s response,
    
4. Saves everything to disk,
    
5. Renders the updated page.
    

  

Let’s go.

---

## **🧾 5.1 What Are Server Actions?**

  

A **Server Action** is an asynchronous function that runs on the server when triggered by a <form></form> or event. You mark it with the 'use server' directive.

  

✅ Benefits:

- No API routes
    
- Automatically invoked by HTML forms
    
- Fully server-side (no client bundle leakage)
    
- Built-in CSRF protection
    

---

## **📁 5.2 Create a Submit Server Action**

  

Create a new file:

app/thread/[id]/actions.ts

```
'use server';

import { addMessageToThread, getThread, saveThread } from '@/lib/threadStore';
import { callLLM } from '@/lib/llm';
import { revalidatePath } from 'next/cache';

export async function submitReply(formData: FormData) {
  const threadId = formData.get('threadId') as string;
  const parentId = formData.get('parentId') as string | null;
  const content = formData.get('content') as string;

  // Step 1: Add user message
  const userMessage = await addMessageToThread(threadId, 'user', content, parentId);

  // Step 2: Get full thread + construct prompt
  const thread = await getThread(threadId);
  const prompt = buildPromptFromThread(thread.messages, userMessage.id);

  // Step 3: Call LLM
  const assistantResponse = await callLLM(prompt);

  // Step 4: Add assistant message
  await addMessageToThread(threadId, 'assistant', assistantResponse, userMessage.id);

  // Step 5: Revalidate page to fetch new data
  revalidatePath(`/thread/${threadId}`);
}
```

Note: buildPromptFromThread was defined in Chapter 3.

---

## **🧑‍💻 5.3 Connect the Form to the Server Action**

  

Update NewReplyForm.tsx to use this server action:

```
'use client';

import { useRef } from 'react';
import { useFormStatus } from 'react-dom';
import { submitReply } from './actions';

type Props = {
  threadId: string;
  parentId: string | null;
};

export function NewReplyForm({ threadId, parentId }: Props) {
  const formRef = useRef<HTMLFormElement>(null);
  const { pending } = useFormStatus();

  return (
    <form
      ref={formRef}
      action={async (formData) => {
        await submitReply(formData);
        formRef.current?.reset();
      }}
      className="space-y-2 mt-2"
    >
      <input type="hidden" name="threadId" value={threadId} />
      <input type="hidden" name="parentId" value={parentId || ''} />
      <textarea
        name="content"
        rows={2}
        required
        className="w-full border p-2 rounded"
        placeholder="Write a reply..."
      />
      <div>
        <button
          type="submit"
          className="text-sm text-blue-600 hover:underline disabled:opacity-50"
          disabled={pending}
        >
          {pending ? 'Sending...' : 'Reply'}
        </button>
      </div>
    </form>
  );
}
```

This does three things:

1. Sends the form data to submitReply on the server.
    
2. Waits for it to complete.
    
3. Resets the form for more replies.
    

---

## **🧪 5.4 Try It Out**

1. Visit a thread page in your browser.
    
2. Type a reply below any message.
    
3. Submit the form.
    
4. Wait a few seconds — the assistant’s reply should appear.
    

  

✅ If you used a local LLM (like Ollama), you’ll see local processing. If you used OpenAI, the cloud takes over.

---

## **🧩 5.5 Enhancements You Can Add Later**

- **Streaming LLM responses** with @vercel/ai (Chapter 9)
    
- **Client transitions** using useTransition to make it snappier
    
- **Optimistic UI** to render your message instantly while waiting
    
- **Global loading indicators** to improve UX during LLM calls
    
- **Error handling** with try/catch and user feedback
    

---

## **✅ Summary**

  

You now have:

- A complete **server-side form submission system** using Server Actions
    
- Dynamic response generation from a local or cloud LLM
    
- Auto-refreshing of thread pages after submission
    

  

You’ve eliminated the need for POST /api/thread, AJAX calls, and client-side data mutations.

---

> In the next chapter, we’ll explore how to plug in **local semantic memory** with embeddings and a vector database like ChromaDB to give your agent **recall** and long-term memory.


---


# **Chapter 6: AI Integration with Local or Hosted Models**

  

This chapter focuses on the brain of your app — the **language model**. You’ll learn how to plug in either a **cloud-hosted model like OpenAI** or a **local LLM via Ollama**, and how to format multi-turn threads into effective prompts.

  

Then, we’ll future-proof our system by creating a modular callLLM() function that supports plug-and-play backends and prepares us for **semantic memory injection** and **agentic flows** later.

---

## **🤔 6.1 Choosing Your LLM Backend**

  

There are two main options:

---

### **✅ Option A:** 

### **OpenAI (Easy, Cloud)**

  

Fastest to start — just sign up and grab an API key.

```
npm install openai
```

In .env.local:

```
OPENAI_API_KEY=sk-...
```

Create /lib/llm.ts:

```
import OpenAI from 'openai';

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

export async function callLLM(messages: { role: string; content: string }[]): Promise<string> {
  const res = await openai.chat.completions.create({
    model: 'gpt-4',
    messages,
  });

  return res.choices[0].message.content || '';
}
```

---

### **✅ Option B:** 

### **Ollama (Local, Private, Free)**

  

Ollama lets you run open-source models like Mistral, LLaMA 3, or Qwen2 on your own hardware. Great for local-first or offline projects.

  

> Install from: https://ollama.com/download

  

Then in your terminal:

```
ollama run mistral
```

Now update callLLM():

```
export async function callLLM(messages: { role: string; content: string }[]): Promise<string> {
  const prompt = messages.map(m => `${m.role}: ${m.content}`).join('\n');

  const res = await fetch('http://localhost:11434/api/generate', {
    method: 'POST',
    body: JSON.stringify({
      model: 'mistral',
      prompt,
      stream: false,
    }),
    headers: { 'Content-Type': 'application/json' }
  });

  const data = await res.json();
  return data.response.trim();
}
```

This approach has:

- No usage limits
    
- No API key
    
- Full privacy and control
    
- ~7B models that work on Mac M1/M2 or Linux
    

---

## **🧠 6.2 Format Messages for Prompt Consistency**

  

Every time a user submits a message, we create a structured prompt like this:

```
[
  { role: 'system', content: 'You are a helpful assistant.' },
  { role: 'user', content: 'What is server-side rendering?' },
  { role: 'assistant', content: 'SSR is when HTML is rendered on the server...' },
  { role: 'user', content: 'How is that different from CSR?' }
]
```

This is done using the buildPromptFromThread() function from Chapter 3, which climbs the parentId chain and flattens the message tree into an LLM-readable list.

  

This allows your model to **maintain continuity** across a multi-turn exchange.

---

## **🔄 6.3 Plug into Submission Pipeline**

  

Your submitReply() Server Action (from Chapter 5) already looks like:

```
export async function submitReply(formData: FormData) {
  const threadId = formData.get('threadId') as string;
  const parentId = formData.get('parentId') as string | null;
  const content = formData.get('content') as string;

  const userMessage = await addMessageToThread(threadId, 'user', content, parentId);
  const thread = await getThread(threadId);
  const prompt = buildPromptFromThread(thread.messages, userMessage.id);
  const assistantResponse = await callLLM(prompt);
  await addMessageToThread(threadId, 'assistant', assistantResponse, userMessage.id);

  revalidatePath(`/thread/${threadId}`);
}
```

That’s all the orchestration needed.

  

✅ The full context goes to the LLM

✅ The LLM reply is saved

✅ The UI updates server-side

---

## **🔁 6.4 Optional: Add Retry & Fallback**

  

If you’re using OpenAI and hit rate limits or timeouts, wrap your call with fallback logic:

```
try {
  return await callLLM(prompt);
} catch (err) {
  console.error('LLM failed', err);
  return 'Sorry, something went wrong generating a reply.';
}
```

---

## **🔗 6.5 Preparing for Agent Chains (Optional)**

  

This callLLM() function is just a basic one-shot completion. Later, you can wrap it in an **agent orchestration graph**.

  

For example:

```
export async function callWithAgents(messages) {
  const reflection = await callLLM([
    { role: 'system', content: 'You are a critic.' },
    ...messages,
    { role: 'user', content: 'Can you identify unclear parts of the above?' }
  ]);

  const summary = await callLLM([
    { role: 'system', content: 'You are a summarizer.' },
    ...messages,
    { role: 'user', content: 'Summarize this conversation.' }
  ]);

  return [reflection, summary];
}
```

This pattern of **multi-agent orchestration** will be formalized in Chapter 10 with graph flows.

---

## **✅ Summary**

  

You now have:

- A working LLM backend — local or cloud
    
- Prompt formatting for multi-turn coherence
    
- Integration with your thread system via Server Actions
    
- A modular callLLM() function ready for future expansion
    

  

Whether you’re using GPT-4, Mistral, or Qwen2, your app now has a brain — and it’s thinking in context.

---

> In the next chapter, you’ll integrate **semantic memory** by embedding past messages and retrieving relevant context — giving your agent “recall” of similar ideas across threads.

  

---

# **Chapter 7: Adding Semantic Memory with Vector Search**

  

Your agent now holds structured multi-turn threads in memory, but it can’t yet recall **related concepts** from outside the current conversation. That’s where **semantic memory** comes in.

  

In this chapter, you’ll learn how to:

- Encode messages as vector embeddings
    
- Store them in a **vector database** (like ChromaDB)
    
- Query them based on **semantic similarity**
    
- Inject relevant results into your LLM prompt for deeper, cross-thread awareness
    

  

This turns your app from a narrow assistant into a **knowledge navigator**, capable of reflecting on your ideas, past answers, or adjacent questions — even across threads.

---

## **🧠 7.1 What Is Semantic Memory?**

  

So far, your thread prompt looks like this:

```
[
  { role: 'user', content: 'What is SSR?' },
  { role: 'assistant', content: 'SSR renders HTML server-side.' },
  { role: 'user', content: 'How is CSR different?' }
]
```

But what if 5 threads ago, you had a similar exchange like:

  

> “Can you explain hydration in React?”

  

Instead of hoping your model remembers or repeats itself, we’ll **retrieve similar past messages**, even from _different_ threads, and inject them into the prompt like:

```
{ role: 'user', content: '[Retrieved memory] React hydration is a related concept to SSR/CSR...' }
```

Now your AI becomes self-consistent and long-term coherent.

---

## **🧬 7.2 Install ChromaDB (Local Vector DB)**

  

ChromaDB is a fast, embeddable, open-source vector database that’s perfect for projects like this.

  

Install:

```
npm install chromadb
```

Then create a file: lib/memory.ts

---

## **✍️ 7.3 Encode Messages as Embeddings**

  

Use sentence-transformers or a remote embedding API to convert text into vectors.

  

### **Option A: Use OpenAI for Embeddings**

```
npm install openai
```

```
import OpenAI from 'openai';

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

export async function embed(text: string): Promise<number[]> {
  const res = await openai.embeddings.create({
    model: 'text-embedding-ada-002',
    input: text
  });

  return res.data[0].embedding;
}
```

### **Option B: Use Ollama for Local Embedding (experimental)**

```
export async function embed(text: string): Promise<number[]> {
  const res = await fetch('http://localhost:11434/api/embeddings', {
    method: 'POST',
    body: JSON.stringify({ model: 'nomic-embed-text', prompt: text }),
    headers: { 'Content-Type': 'application/json' },
  });
  const data = await res.json();
  return data.embedding;
}
```

Now you can convert any message content into a vector.

---

## **📥 7.4 Store Embeddings in ChromaDB**

  

In lib/memory.ts:

```
import { ChromaClient } from 'chromadb';

const chroma = new ChromaClient();
const collection = await chroma.getOrCreateCollection({ name: 'messages' });

export async function storeMessageEmbedding({
  id,
  content,
  metadata
}: {
  id: string;
  content: string;
  metadata: any;
}) {
  const embedding = await embed(content);
  await collection.add({
    ids: [id],
    embeddings: [embedding],
    metadatas: [metadata],
    documents: [content],
  });
}
```

Call this **after each new message is saved**, like:

```
await storeMessageEmbedding({
  id: message.id,
  content: message.content,
  metadata: {
    role: message.role,
    threadId,
    parentId: message.parentId,
  }
});
```

---

## **🔎 7.5 Query Related Messages**

  

Add this to lib/memory.ts:

```
export async function retrieveRelevantMemory(query: string, k = 3) {
  const embedding = await embed(query);
  const results = await collection.query({
    queryEmbeddings: [embedding],
    nResults: k,
  });

  return results.documents[0]; // array of top k documents
}
```

Use this before calling the LLM:

```
const related = await retrieveRelevantMemory(userMessage.content);

const memoryMessages = related.map(content => ({
  role: 'user',
  content: `[Related memory] ${content}`
}));
```

Then **prepend** this to your prompt array:

```
const prompt = [
  { role: 'system', content: 'You are a helpful assistant.' },
  ...memoryMessages,
  ...chainFromThreadHistory
];
```

The LLM now has extra awareness of similar ideas — even if they happened in other threads, weeks ago.

---

## **🧪 7.6 Test Recall in Context**

1. Ask a question in Thread A:
    
    “What is React hydration?”
    
2. Ask a related question in Thread B:
    
    “How is SSR related to hydration?”
    
3. The assistant should now use **semantic memory** to answer with more insight — referencing Thread A.
    

---

## **✅ Summary**

  

You’ve added true **recall** to your assistant by:

- Embedding all messages as semantic vectors
    
- Querying similar content based on a new input
    
- Prepending relevant past content to the LLM prompt
    

  

This makes your Q&A app more than a forum — it’s a **persistent knowledge graph**, where each new question strengthens the system’s understanding.

---

> Next, we’ll polish the frontend: building a clean, deeply nested message UI that allows interactive replies.

  
---

# **Chapter 8: Building the Threaded UI**

  

Now that your backend is equipped with semantic memory and your AI can generate contextually aware replies, it’s time to focus on **the user interface**. In this chapter, you’ll:

- Render deeply nested conversations clearly
    
- Improve the visual hierarchy between messages
    
- Let users reply to any message, not just the original question
    
- Enhance the UX with form placement and clean layout
    
- Prepare for future expansions like markdown and streaming
    

  

This UI will serve as the front-facing visualization of your knowledge graph.

---

## **🧱 8.1 Message Tree Rendering (Refresher)**

  

As built in Chapter 4, each thread consists of messages with parentId pointers. Rendering the UI is done via a recursive component:

```
// ThreadMessage.tsx
export function ThreadMessage({ message, allMessages, threadId, depth = 0 }: Props) {
  const replies = allMessages.filter(m => m.parentId === message.id);

  return (
    <div className={`ml-${depth * 4} border-l border-gray-300 pl-4 py-2`}>
      <div className={`p-2 rounded ${message.role === 'user' ? 'bg-blue-50' : 'bg-gray-100'}`}>
        <span className="font-semibold">{message.role}</span>: {message.content}
      </div>

      <div className="ml-2 mt-1">
        <NewReplyForm threadId={threadId} parentId={message.id} />
      </div>

      {replies.map(reply => (
        <ThreadMessage
          key={reply.id}
          message={reply}
          allMessages={allMessages}
          threadId={threadId}
          depth={depth + 1}
        />
      ))}
    </div>
  );
}
```

✅ This renders:

- Replies under their parents
    
- Form to respond to any message
    
- Indented visual hierarchy
    

---

## **🎨 8.2 Styling Guidelines**

  

Use consistent visual cues:

- **User messages**: blue or white background
    
- **Assistant replies**: gray or light-yellow
    
- **Nesting**: increase left margin with depth
    
- **Fonts**: differentiate user/assistant with bold or italics
    
- **Reply input**: compact textarea under each message
    

  

Example styles:

```
const roleColor =
  message.role === 'user' ? 'bg-blue-50' : 'bg-yellow-50';

return (
  <div className={`ml-${depth * 4} border-l pl-4 py-2`}>
    <div className={`p-2 rounded shadow-sm ${roleColor}`}>
      <span className="text-sm font-semibold capitalize">
        {message.role}
      </span>
      <div className="mt-1 whitespace-pre-wrap">{message.content}</div>
    </div>
    ...
  </div>
);
```

---

## **✏️ 8.3 Reply Forms: Micro or Macro?**

  

Two reply input UX options:

  

### **1.** 

### **Micro Form (per message)**

- Inline reply box beneath each message
    
- Pros: immediate, intuitive
    
- Cons: cluttered if too many
    

  

### **2.** 

### **Macro Form (bottom of page)**

- Single input for the entire thread
    
- Pros: clean UI
    
- Cons: less precise in reply targeting
    

  

Choose what suits your app best — or offer both.

---

## **🧠 8.4 Show Semantic Memory Matches (Optional)**

  

If semantic memory returns results (from Chapter 7), you can **display them in a sidebar or above the form**.

  

Example:

```
{retrievedMemories.length > 0 && (
  <div className="mb-4 p-3 bg-purple-50 border rounded">
    <h3 className="text-sm font-semibold">Context from Memory</h3>
    <ul className="text-sm mt-1 list-disc ml-4">
      {retrievedMemories.map((text, i) => (
        <li key={i}>{text}</li>
      ))}
    </ul>
  </div>
)}
```

This helps users see what’s influencing the AI’s reply — and builds trust.

---

## **🧑‍🎨 8.5 UX Tweaks**

  

Small details that improve the feel:

- **Timestamp**: Add new Date(msg.timestamp).toLocaleString() below each bubble
    
- **Hover effects**: Lighten background on hover for focus
    
- **Thread title**: Render prominently at the top
    
- **Empty thread state**: “No replies yet — be the first to ask!”
    

---

## **📚 8.6 Preparing for Markdown & Code Blocks**

  

AI answers often include Markdown or code. Install a library to handle this:

```
npm install react-markdown
```

Then in your ThreadMessage:

```
import ReactMarkdown from 'react-markdown';

<ReactMarkdown className="prose">{message.content}</ReactMarkdown>
```

You can also style code and pre blocks for better readability.

---

## **✅ Summary**

  

You now have:

- A recursive threaded UI that handles any reply depth
    
- Clean styling to distinguish user vs assistant roles
    
- Embedded forms to post follow-ups inline
    
- The option to preview retrieved memory or markdown
    

  

This chapter solidifies your app as a **readable, writable knowledge forum**, not just a chat UI.

---

> Next, we’ll expand this interface with **real-time features**: markdown, streaming replies, user accounts, home pages, and more.

  
---


# **Chapter 9: Bonus Features and UX Enhancements**

  

Now that your AI Q&A forum is fully functional — with multi-turn threads, local or cloud-based LLM replies, semantic memory, and nested UI — it’s time to level up.

  

In this chapter, we’ll cover a suite of powerful upgrades:

- 🧾 Markdown rendering and syntax highlighting
    
- 🔁 Streaming AI responses
    
- 👤 Basic user identity support
    
- 🏠 A home page listing all threads
    
- 🔍 Search and summarization
    
- ⏳ Pagination or infinite scroll
    
- 🔗 Thread permalinks and URLs
    

  

Each one helps polish your app from prototype into something demo-worthy, scalable, and even commercially viable.

---

## **🧾 9.1 Markdown Rendering**

  

We already installed react-markdown in Chapter 8. Let’s enhance it.

  

Install a code highlighter:

```
npm install react-syntax-highlighter
```

Update ThreadMessage.tsx:

```
import ReactMarkdown from 'react-markdown';
import { Prism as SyntaxHighlighter } from 'react-syntax-highlighter';
import { materialDark } from 'react-syntax-highlighter/dist/cjs/styles/prism';

<ReactMarkdown
  className="prose max-w-none"
  components={{
    code({ inline, className, children }) {
      const match = /language-(\w+)/.exec(className || '');
      return !inline ? (
        <SyntaxHighlighter
          language={match?.[1] || 'text'}
          style={materialDark}
        >
          {String(children).replace(/\n$/, '')}
        </SyntaxHighlighter>
      ) : (
        <code className={className}>{children}</code>
      );
    }
  }}
>
  {message.content}
</ReactMarkdown>
```

✅ Now AI responses with code blocks will be beautifully rendered.

---

## **🔁 9.2 Streaming AI Responses (Vercel AI SDK)**

  

To show responses as they generate, install:

```
npm install ai
```

Then update your submitReply() flow to **return a stream** instead of a full string:

```
// app/api/stream/route.ts
import { OpenAIStream, StreamingTextResponse } from 'ai';
import OpenAI from 'openai';

const openai = new OpenAI();

export async function POST(req: Request) {
  const { messages } = await req.json();

  const response = await openai.chat.completions.create({
    model: 'gpt-4',
    stream: true,
    messages
  });

  const stream = OpenAIStream(response);
  return new StreamingTextResponse(stream);
}
```

On the client:

```
import { useChat } from 'ai/react';

const { messages, handleInputChange, handleSubmit } = useChat({
  api: '/api/stream'
});
```

This shows messages being typed out as the LLM replies. Combine this with suspense or loading skeletons for best UX.

---

## **👤 9.3 Add Basic Identity Support**

  

You can identify users by:

- GitHub OAuth (with next-auth)
    
- Anonymous session ID
    
- Email/password login
    

  

To start, use next-auth:

```
npm install next-auth
```

Then in /app/api/auth/[...nextauth]/route.ts, configure providers. Use getServerSession() to show a “Posted by You” badge.

---

## **🏠 9.4 Home Page Listing Threads**

  

In app/page.tsx:

```
import fs from 'fs/promises';
import path from 'path';

export default async function HomePage() {
  const files = await fs.readdir('data/threads');
  const threads = await Promise.all(files.map(async file => {
    const raw = await fs.readFile(`data/threads/${file}`, 'utf8');
    return JSON.parse(raw);
  }));

  return (
    <div className="max-w-xl mx-auto p-6">
      <h1 className="text-2xl font-bold mb-4">🧵 Threads</h1>
      <ul className="space-y-3">
        {threads.map(thread => (
          <li key={thread.id}>
            <a href={`/thread/${thread.id}`} className="text-blue-600 underline">
              {thread.title}
            </a>
          </li>
        ))}
      </ul>
    </div>
  );
}
```

✅ Simple thread index with clickable links.

---

## **🔍 9.5 Search & Semantic Query (Optional)**

  

To search by **similar meaning**, embed the search string and query ChromaDB:

```
const results = await chroma.query({
  queryEmbeddings: [await embed('What is SSR')],
  nResults: 5
});
```

Then return the matching messages or threads.

---

## **⏳ 9.6 Pagination & Lazy Loading**

  

For threads with dozens of replies, add pagination:

- Slice the message list (messages.slice(0, 20))
    
- Add a “Load more replies” button
    
- Use React Server Components or streaming to progressively reveal children
    

---

## **🔗 9.7 Permalinks for Replies**

  

Add fragment links like /thread/[id]#msg-[message.id]

In your render function:

```
<a id={`msg-${message.id}`} />
```

Then let users “copy link to reply” for reference, or even quote previous messages in new threads.

---

## **✅ Summary**

  

You’ve now equipped your app with:

- Beautiful markdown + code formatting
    
- Live streaming replies
    
- User session support
    
- Full thread listing
    
- Search and context recall
    
- Pagination and cleaner UI
    
- Stable permalinks for referencing ideas
    

---

> Next, we’ll extend the backend into an **agentic architecture**: chaining specialized agents like reflectors, summarizers, and critics using graph logic.

  

## Chapter 10: Agentic Graph Expansion (Optional)

In this chapter, you'll transform your single-assistant system into a multi-agent orchestration platform. Instead of one AI generating responses, you'll create specialized agents that work together in a graph-based workflow — enabling sophisticated reasoning patterns like reflection, criticism, and iterative improvement.

### 🕸 10.1 Understanding Agentic Graphs

An agentic graph treats each AI task as a node in a workflow. Instead of:

```
User Question → Single LLM → Response
```

You'll create:

```
User Question → Generator → Critic → Refiner → Summarizer → Response
```

Each agent has a specific role:

- **Generator**: Creates initial responses
- **Critic**: Evaluates and identifies weaknesses
- **Refiner**: Improves based on criticism
- **Summarizer**: Distills key insights
- **Reflector**: Connects to broader themes

### 🏗 10.2 Agent Node Architecture

Create `/lib/agents/base.ts`:

typescript

```typescript
export type AgentNode = {
  id: string;
  name: string;
  systemPrompt: string;
  inputs: string[];
  outputs: string[];
  execute: (inputs: Record<string, string>) => Promise<string>;
};

export abstract class BaseAgent {
  abstract id: string;
  abstract name: string;
  abstract systemPrompt: string;
  
  async execute(inputs: Record<string, string>): Promise<string> {
    const messages = [
      { role: 'system', content: this.systemPrompt },
      { role: 'user', content: this.formatInputs(inputs) }
    ];
    
    return await callLLM(messages);
  }
  
  protected abstract formatInputs(inputs: Record<string, string>): string;
}
```

### 🎯 10.3 Specialized Agent Implementations

Create `/lib/agents/generator.ts`:

typescript

```typescript
import { BaseAgent } from './base';

export class GeneratorAgent extends BaseAgent {
  id = 'generator';
  name = 'Response Generator';
  systemPrompt = `You are a thoughtful assistant who provides comprehensive, well-structured answers. Focus on clarity, accuracy, and depth. Consider multiple perspectives when relevant.`;
  
  protected formatInputs(inputs: Record<string, string>): string {
    const { question, context = '' } = inputs;
    return `Question: ${question}\n\nContext: ${context}\n\nPlease provide a thorough response.`;
  }
}
```

Create `/lib/agents/critic.ts`:

typescript

```typescript
import { BaseAgent } from './base';

export class CriticAgent extends BaseAgent {
  id = 'critic';
  name = 'Response Critic';
  systemPrompt = `You are a critical analyst who evaluates responses for accuracy, completeness, and clarity. Identify specific areas for improvement without being overly harsh.`;
  
  protected formatInputs(inputs: Record<string, string>): string {
    const { question, response } = inputs;
    return `Original Question: ${question}\n\nResponse to evaluate: ${response}\n\nProvide specific, constructive feedback on how this response could be improved.`;
  }
}
```

Create `/lib/agents/refiner.ts`:

typescript

```typescript
import { BaseAgent } from './base';

export class RefinerAgent extends BaseAgent {
  id = 'refiner';
  name = 'Response Refiner';
  systemPrompt = `You are a skilled editor who improves responses based on feedback. Maintain the original intent while addressing identified weaknesses.`;
  
  protected formatInputs(inputs: Record<string, string>): string {
    const { question, originalResponse, feedback } = inputs;
    return `Question: ${question}\n\nOriginal Response: ${originalResponse}\n\nFeedback: ${feedback}\n\nPlease provide an improved response that addresses the feedback.`;
  }
}
```

Create `/lib/agents/summarizer.ts`:

typescript

```typescript
import { BaseAgent } from './base';

export class SummarizerAgent extends BaseAgent {
  id = 'summarizer';
  name = 'Thread Summarizer';
  systemPrompt = `You are a concise summarizer who distills complex discussions into key insights and actionable takeaways.`;
  
  protected formatInputs(inputs: Record<string, string>): string {
    const { threadContent } = inputs;
    return `Thread Content: ${threadContent}\n\nProvide a concise summary highlighting the main insights and conclusions.`;
  }
}
```

### 🔄 10.4 Agent Orchestration Engine

Create `/lib/agents/orchestrator.ts`:

typescript

```typescript
import { BaseAgent } from './base';

export type AgentFlow = {
  id: string;
  name: string;
  nodes: AgentNode[];
  edges: AgentEdge[];
};

export type AgentEdge = {
  from: string;
  to: string;
  inputMapping: Record<string, string>;
};

export class AgentOrchestrator {
  private agents: Map<string, BaseAgent> = new Map();
  
  registerAgent(agent: BaseAgent) {
    this.agents.set(agent.id, agent);
  }
  
  async executeFlow(flow: AgentFlow, initialInputs: Record<string, string>): Promise<Record<string, string>> {
    const results: Record<string, string> = { ...initialInputs };
    const executed: Set<string> = new Set();
    
    // Topological sort and execute
    const sortedNodes = this.topologicalSort(flow);
    
    for (const nodeId of sortedNodes) {
      const agent = this.agents.get(nodeId);
      if (!agent) continue;
      
      const nodeInputs = this.prepareNodeInputs(nodeId, flow.edges, results);
      const output = await agent.execute(nodeInputs);
      
      results[nodeId] = output;
      executed.add(nodeId);
    }
    
    return results;
  }
  
  private topologicalSort(flow: AgentFlow): string[] {
    // Implement topological sort for dependency resolution
    const inDegree: Record<string, number> = {};
    const adjList: Record<string, string[]> = {};
    
    // Initialize
    flow.nodes.forEach(node => {
      inDegree[node.id] = 0;
      adjList[node.id] = [];
    });
    
    // Build graph
    flow.edges.forEach(edge => {
      adjList[edge.from].push(edge.to);
      inDegree[edge.to]++;
    });
    
    const queue: string[] = [];
    Object.keys(inDegree).forEach(nodeId => {
      if (inDegree[nodeId] === 0) {
        queue.push(nodeId);
      }
    });
    
    const result: string[] = [];
    while (queue.length > 0) {
      const current = queue.shift()!;
      result.push(current);
      
      adjList[current].forEach(neighbor => {
        inDegree[neighbor]--;
        if (inDegree[neighbor] === 0) {
          queue.push(neighbor);
        }
      });
    }
    
    return result;
  }
  
  private prepareNodeInputs(nodeId: string, edges: AgentEdge[], results: Record<string, string>): Record<string, string> {
    const inputs: Record<string, string> = {};
    
    edges.forEach(edge => {
      if (edge.to === nodeId) {
        Object.entries(edge.inputMapping).forEach(([inputKey, resultKey]) => {
          inputs[inputKey] = results[resultKey];
        });
      }
    });
    
    return inputs;
  }
}
```

### 🎨 10.5 Visual Flow Builder with ReactFlow

Install ReactFlow for graph visualization:

bash

```bash
npm install reactflow
```

Create `/components/FlowBuilder.tsx`:

typescript

```typescript
'use client';

import React, { useCallback, useState } from 'react';
import ReactFlow, {
  Node,
  Edge,
  addEdge,
  Connection,
  useNodesState,
  useEdgesState,
  Controls,
  Background,
} from 'reactflow';
import 'reactflow/dist/style.css';

const initialNodes: Node[] = [
  {
    id: 'user',
    type: 'input',
    position: { x: 0, y: 0 },
    data: { label: 'User Question' },
  },
  {
    id: 'generator',
    position: { x: 200, y: 0 },
    data: { label: 'Generator' },
  },
  {
    id: 'critic',
    position: { x: 400, y: 0 },
    data: { label: 'Critic' },
  },
  {
    id: 'refiner',
    position: { x: 600, y: 0 },
    data: { label: 'Refiner' },
  },
  {
    id: 'output',
    type: 'output',
    position: { x: 800, y: 0 },
    data: { label: 'Final Response' },
  },
];

const initialEdges: Edge[] = [
  { id: 'user-generator', source: 'user', target: 'generator' },
  { id: 'generator-critic', source: 'generator', target: 'critic' },
  { id: 'critic-refiner', source: 'critic', target: 'refiner' },
  { id: 'refiner-output', source: 'refiner', target: 'output' },
];

export function FlowBuilder() {
  const [nodes, setNodes, onNodesChange] = useNodesState(initialNodes);
  const [edges, setEdges, onEdgesChange] = useEdgesState(initialEdges);

  const onConnect = useCallback(
    (params: Connection) => setEdges((eds) => addEdge(params, eds)),
    [setEdges]
  );

  return (
    <div className="w-full h-96 border rounded-lg">
      <ReactFlow
        nodes={nodes}
        edges={edges}
        onNodesChange={onNodesChange}
        onEdgesChange={onEdgesChange}
        onConnect={onConnect}
        fitView
      >
        <Controls />
        <Background />
      </ReactFlow>
    </div>
  );
}
```

### 🔧 10.6 Integrating Agent Flows into Threads

Update your Server Action to use agent orchestration:

typescript

```typescript
// app/thread/[id]/actions.ts
'use server';

import { AgentOrchestrator } from '@/lib/agents/orchestrator';
import { GeneratorAgent } from '@/lib/agents/generator';
import { CriticAgent } from '@/lib/agents/critic';
import { RefinerAgent } from '@/lib/agents/refiner';

const orchestrator = new AgentOrchestrator();
orchestrator.registerAgent(new GeneratorAgent());
orchestrator.registerAgent(new CriticAgent());
orchestrator.registerAgent(new RefinerAgent());

export async function submitReplyWithAgents(formData: FormData) {
  const threadId = formData.get('threadId') as string;
  const parentId = formData.get('parentId') as string | null;
  const content = formData.get('content') as string;
  
  // Add user message
  const userMessage = await addMessageToThread(threadId, 'user', content, parentId);
  
  // Get context
  const thread = await getThread(threadId);
  const context = buildPromptFromThread(thread.messages, userMessage.id);
  
  // Execute agent flow
  const flow = {
    id: 'default',
    name: 'Generate-Critique-Refine',
    nodes: [
      { id: 'generator', name: 'Generator', systemPrompt: '', inputs: [], outputs: [] },
      { id: 'critic', name: 'Critic', systemPrompt: '', inputs: [], outputs: [] },
      { id: 'refiner', name: 'Refiner', systemPrompt: '', inputs: [], outputs: [] },
    ],
    edges: [
      { from: 'generator', to: 'critic', inputMapping: { question: 'question', response: 'generator' } },
      { from: 'critic', to: 'refiner', inputMapping: { question: 'question', originalResponse: 'generator', feedback: 'critic' } },
    ],
  };
  
  const results = await orchestrator.executeFlow(flow, {
    question: content,
    context: context.map(m => `${m.role}: ${m.content}`).join('\n'),
  });
  
  const finalResponse = results.refiner || results.generator;
  
  // Save assistant message
  await addMessageToThread(threadId, 'assistant', finalResponse, userMessage.id);
  
  revalidatePath(`/thread/${threadId}`);
}
```

### 🧪 10.7 Testing Agent Flows

Create a test page at `/app/agents/test/page.tsx`:

typescript

```typescript
import { AgentOrchestrator } from '@/lib/agents/orchestrator';
import { GeneratorAgent } from '@/lib/agents/generator';
import { CriticAgent } from '@/lib/agents/critic';

export default async function AgentTestPage() {
  const orchestrator = new AgentOrchestrator();
  orchestrator.registerAgent(new GeneratorAgent());
  orchestrator.registerAgent(new CriticAgent());
  
  const flow = {
    id: 'test',
    name: 'Test Flow',
    nodes: [
      { id: 'generator', name: 'Generator', systemPrompt: '', inputs: [], outputs: [] },
      { id: 'critic', name: 'Critic', systemPrompt: '', inputs: [], outputs: [] },
    ],
    edges: [
      { from: 'generator', to: 'critic', inputMapping: { question: 'question', response: 'generator' } },
    ],
  };
  
  const results = await orchestrator.executeFlow(flow, {
    question: 'What is the future of AI?',
    context: '',
  });
  
  return (
    <div className="max-w-4xl mx-auto p-6">
      <h1 className="text-2xl font-bold mb-6">Agent Flow Test</h1>
      
      <div className="space-y-4">
        <div className="p-4 bg-blue-50 rounded">
          <h3 className="font-semibold">Generator Output:</h3>
          <p className="mt-2">{results.generator}</p>
        </div>
        
        <div className="p-4 bg-yellow-50 rounded">
          <h3 className="font-semibold">Critic Output:</h3>
          <p className="mt-2">{results.critic}</p>
        </div>
      </div>
    </div>
  );
}
```

### ✅ Summary

You've now built:

- **Modular agent architecture** with specialized roles
- **Graph-based orchestration** for complex reasoning workflows
- **Visual flow builder** using ReactFlow
- **Integration with existing thread system**
- **Testing framework** for agent flows

This transforms your Q&A forum into a sophisticated reasoning platform where AI agents collaborate to provide thoughtful, well-analyzed responses.

---

## Chapter 11: Deployment & Local-First LLMs

This chapter covers deploying your AI-powered Q&A forum in two scenarios: cloud deployment via Vercel for scalability, and local-first deployment for privacy and control. You'll learn optimization techniques, model selection strategies, and how to create a fully offline-capable system.

### 🚀 11.1 Cloud Deployment with Vercel

#### Environment Setup

For production deployment, configure your environment variables:

bash

```bash
# .env.production
OPENAI_API_KEY=sk-...
CHROMA_HOST=https://your-chroma-instance.com
DATABASE_URL=postgresql://...
NEXTAUTH_SECRET=your-secret-key
NEXTAUTH_URL=https://yourdomain.com
```

#### Vercel Configuration

Create `vercel.json`:

json

```json
{
  "functions": {
    "app/api/*/route.ts": {
      "maxDuration": 30
    }
  },
  "env": {
    "OPENAI_API_KEY": "@openai_api_key",
    "CHROMA_HOST": "@chroma_host"
  },
  "build": {
    "env": {
      "NEXT_TELEMETRY_DISABLED": "1"
    }
  }
}
```

#### Database Migration Strategy

Since file-based storage won't work in serverless, migrate to a database:

typescript

```typescript
// lib/db.ts
import { createClient } from '@supabase/supabase-js';

const supabase = createClient(
  process.env.NEXT_PUBLIC_SUPABASE_URL!,
  process.env.SUPABASE_SERVICE_ROLE_KEY!
);

export async function getThread(id: string) {
  const { data } = await supabase
    .from('threads')
    .select('*')
    .eq('id', id)
    .single();
  return data;
}

export async function saveThread(thread: any) {
  await supabase
    .from('threads')
    .upsert(thread);
}

export async function getMessages(threadId: string) {
  const { data } = await supabase
    .from('messages')
    .select('*')
    .eq('thread_id', threadId)
    .order('timestamp', { ascending: true });
  return data;
}
```

#### Optimized LLM Integration

For production, implement request batching and caching:

typescript

```typescript
// lib/llm-optimized.ts
import { OpenAI } from 'openai';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// Response cache for identical prompts
const responseCache = new Map<string, { response: string; timestamp: number }>();
const CACHE_TTL = 5 * 60 * 1000; // 5 minutes

export async function callLLMWithCache(messages: any[]): Promise<string> {
  const cacheKey = JSON.stringify(messages);
  const cached = responseCache.get(cacheKey);
  
  if (cached && Date.now() - cached.timestamp < CACHE_TTL) {
    return cached.response;
  }
  
  const response = await openai.chat.completions.create({
    model: 'gpt-4-turbo',
    messages,
    temperature: 0.7,
    max_tokens: 1000,
  });
  
  const result = response.choices[0].message.content || '';
  responseCache.set(cacheKey, { response: result, timestamp: Date.now() });
  
  return result;
}
```

#### Deploy Command

bash

```bash
# Install Vercel CLI
npm install -g vercel

# Deploy
vercel --prod
```

### 🏠 11.2 Local-First Architecture

#### Complete Local Stack

For a fully local deployment, use this architecture:

```
┌─────────────────────────────────────────────────┐
│                Next.js App                      │
├─────────────────────────────────────────────────┤
│  SQLite Database    │    ChromaDB Vector Store  │
├─────────────────────────────────────────────────┤
│            Ollama LLM Server                    │
└─────────────────────────────────────────────────┘
```

#### SQLite Setup

Install dependencies:

bash

```bash
npm install sqlite3 @types/sqlite3
```

Create `/lib/sqlite.ts`:

typescript

```typescript
import sqlite3 from 'sqlite3';
import { open } from 'sqlite';

let db: any = null;

export async function getDb() {
  if (!db) {
    db = await open({
      filename: './data/database.db',
      driver: sqlite3.Database,
    });
    
    // Initialize tables
    await db.exec(`
      CREATE TABLE IF NOT EXISTS threads (
        id TEXT PRIMARY KEY,
        title TEXT,
        created_at INTEGER
      );
      
      CREATE TABLE IF NOT EXISTS messages (
        id TEXT PRIMARY KEY,
        thread_id TEXT,
        parent_id TEXT,
        role TEXT,
        content TEXT,
        timestamp INTEGER,
        FOREIGN KEY (thread_id) REFERENCES threads (id)
      );
      
      CREATE INDEX IF NOT EXISTS idx_messages_thread_id ON messages(thread_id);
      CREATE INDEX IF NOT EXISTS idx_messages_parent_id ON messages(parent_id);
    `);
  }
  
  return db;
}

export async function saveThreadSQLite(thread: any) {
  const db = await getDb();
  
  await db.run(`
    INSERT OR REPLACE INTO threads (id, title, created_at)
    VALUES (?, ?, ?)
  `, [thread.id, thread.title, Date.now()]);
  
  // Save messages
  for (const message of thread.messages) {
    await db.run(`
      INSERT OR REPLACE INTO messages (id, thread_id, parent_id, role, content, timestamp)
      VALUES (?, ?, ?, ?, ?, ?)
    `, [message.id, thread.id, message.parentId, message.role, message.content, message.timestamp]);
  }
}
```

#### Local ChromaDB Setup

Create a Docker setup for ChromaDB:

yaml

```yaml
# docker-compose.yml
version: '3.8'
services:
  chromadb:
    image: chromadb/chroma:latest
    ports:
      - "8000:8000"
    volumes:
      - ./chroma-data:/chroma/chroma
    environment:
      - CHROMA_SERVER_HOST=0.0.0.0
```

Start it:

bash

```bash
docker-compose up -d
```

#### Ollama Integration

Create `/lib/ollama.ts`:

typescript

```typescript
interface OllamaResponse {
  response: string;
  done: boolean;
}

export async function callOllama(
  messages: { role: string; content: string }[],
  model: string = 'mistral:7b'
): Promise<string> {
  const prompt = messages.map(m => `${m.role}: ${m.content}`).join('\n') + '\nassistant: ';
  
  const response = await fetch('http://localhost:11434/api/generate', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      model,
      prompt,
      stream: false,
      options: {
        temperature: 0.7,
        top_p: 0.9,
        top_k: 40,
      },
    }),
  });
  
  const data: OllamaResponse = await response.json();
  return data.response.trim();
}

export async function callOllamaStream(
  messages: { role: string; content: string }[],
  model: string = 'mistral:7b'
): Promise<ReadableStream> {
  const prompt = messages.map(m => `${m.role}: ${m.content}`).join('\n') + '\nassistant: ';
  
  const response = await fetch('http://localhost:11434/api/generate', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      model,
      prompt,
      stream: true,
    }),
  });
  
  return response.body!;
}
```

### ⚡ 11.3 Performance Optimization

#### Model Selection Guide

**For Local Development:**

- **Mistral 7B**: Fast, good quality, 4-6GB RAM
- **Llama 3 8B**: Higher quality, 6-8GB RAM
- **Qwen2 7B**: Excellent for coding, 4-6GB RAM
- **Phi-3**: Ultra-fast, 2GB RAM, good for simple tasks

**For Production:**

- **GPT-4 Turbo**: Highest quality, expensive
- **Claude 3 Haiku**: Fast, cost-effective
- **Gemini Pro**: Good balance of speed/quality

#### Quantization for Local Models

Use quantized versions for better performance:

bash

```bash
# Install quantized models
ollama run mistral:7b-instruct-q4_0
ollama run llama3:8b-instruct-q4_0
```

#### Caching Strategy

Implement multi-level caching:

typescript

```typescript
// lib/cache.ts
import { LRUCache } from 'lru-cache';

const promptCache = new LRUCache<string, string>({
  max: 1000,
  ttl: 1000 * 60 * 15, // 15 minutes
});

const embeddingCache = new LRUCache<string, number[]>({
  max: 5000,
  ttl: 1000 * 60 * 60, // 1 hour
});

export { promptCache, embeddingCache };
```

#### Database Optimization

Add indexes for better query performance:

sql

```sql
-- For SQLite
CREATE INDEX idx_messages_timestamp ON messages(timestamp);
CREATE INDEX idx_messages_role ON messages(role);
CREATE INDEX idx_threads_created_at ON threads(created_at);

-- For faster semantic search
CREATE INDEX idx_messages_content_fts ON messages USING GIN (to_tsvector('english', content));
```

### 🐳 11.4 Containerized Deployment

Create a complete Docker setup:

dockerfile

```dockerfile
# Dockerfile
FROM node:18-alpine

WORKDIR /app

COPY package*.json ./
RUN npm ci --only=production

COPY . .
RUN npm run build

EXPOSE 3000

CMD ["npm", "start"]
```

Complete `docker-compose.yml`:

yaml

```yaml
version: '3.8'
services:
  app:
    build: .
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=production
      - CHROMA_HOST=http://chromadb:8000
    depends_on:
      - chromadb
    volumes:
      - ./data:/app/data
  
  chromadb:
    image: chromadb/chroma:latest
    ports:
      - "8000:8000"
    volumes:
      - ./chroma-data:/chroma/chroma
  
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ./ollama-data:/root/.ollama
```

### 🔧 11.5 Local Development Script

Create a setup script for easy local development:

bash

```bash
#!/bin/bash
# setup-local.sh

echo "Setting up local AI Q&A forum..."

# Create directories
mkdir -p data/threads
mkdir -p chroma-data
mkdir -p ollama-data

# Start services
docker-compose up -d chromadb ollama

# Wait for services
echo "Waiting for services to start..."
sleep 10

# Download models
docker exec ollama-container ollama pull mistral:7b-instruct-q4_0
docker exec ollama-container ollama pull nomic-embed-text

# Install dependencies
npm install

# Create database
npm run db:migrate

echo "Local setup complete! Run 'npm run dev' to start."
```

### 📊 11.6 Monitoring and Analytics

Add basic monitoring:

typescript

```typescript
// lib/analytics.ts
export class Analytics {
  static async trackQuery(query: string, responseTime: number) {
    // Log to file or external service
    console.log(`Query: "${query.substring(0, 50)}..." took ${responseTime}ms`);
  }
  
  static async trackError(error: Error, context: string) {
    console.error(`Error in ${context}:`, error);
  }
  
  static async trackModelUsage(model: string, tokenCount: number) {
    // Track model usage for cost optimization
    console.log(`Model ${model} used ${tokenCount} tokens`);
  }
}
```

### ✅ Summary

You now have comprehensive deployment options:

- **Cloud deployment** with Vercel, Supabase, and OpenAI
- **Local-first architecture** with SQLite, ChromaDB, and Ollama
- **Performance optimization** strategies
- **Containerized deployment** with Docker
- **Monitoring and analytics** setup

Choose the deployment strategy that best fits your privacy, performance, and cost requirements.



---

### Chapter 12: Future Directions  
*Expanding Your AI-Powered Thought Infrastructure*

Your Q&A forum is now a fully functional thinking assistant – but the journey toward truly adaptive AI collaboration has just begun. Here are research-inspired frontiers to explore:

1. **Journaling Integration**  
   ```tsx
   // Example: Auto-summarize daily entries
   const dailySummary = await summarizerAgent.execute({
     threadContent: journalEntries
   });
   ```
   - Connect threads to personal journals
   - Implement timeline views showing idea evolution
   - Auto-generate reflection prompts from unanswered questions

2. **Emotion-Based Memory**  
   ```tsx
   // Emotion-aware response modifier
   const emotionalContext = analyzeSentiment(userMessage.content);
   if(emotionalContext.intensity > 0.8) {
     prompt.push({role: 'system', content: 'User seems frustrated...'})
   }
   ```
   - Augment semantic memory with sentiment analysis
   - Route emotional queries to specialized "empathy agents"
   - Adjust response tone based on detected emotional state

3. **Biometric Feedback Loops**  
   ```tsx
   // Wearable integration concept
   const focusLevel = await getNeurofeedback();
   if(focusLevel < 40) {
     return await summarizerAgent.execute(currentThread);
   }
   ```
   - Integrate with wearables (EEG, HRV) to detect cognitive states
   - Shorten responses during low-attention periods
   - Trigger memory consolidation during high-focus states

4. **Persona Evolution**  
   ```tsx
   // Adaptive persona prompt
   const learningStyle = getUserLearningProfile();
   const systemPrompt = `Explain like I'm ${learningStyle} learner...`;
   ```
   - Create AI personas that evolve with user interaction patterns
   - Develop specialization based on frequently discussed topics
   - Implement "persona snapshots" for different thinking modes

5. **Adaptive Thought Frameworks**  
   ```tsx
   // Dynamic agent selection
   const topic = classifyQuestion(userInput);
   const agent = agentRegistry.getExpertAgentFor(topic);
   ```
   - Automatically switch reasoning frameworks (SWOT, First Principles)
   - Generate custom visualizations for complex concepts
   - Develop problem-specific agent teams on demand

*Research Connections*: These directions draw from agentic knowledge graph research, cognitive architecture papers, and emerging neuroadaptive interfaces. The core principle: **Transform your tool from an answer engine into a thought-partner that evolves with you.**

---

### Appendix: Code Snippets & APIs  
*Developer Cheat Sheet*

#### Form Actions with Server Components
```tsx
// app/thread/[id]/actions.ts
'use server';
export async function submitReply(formData: FormData) {
  const content = formData.get('content') as string;
  // ...logic
  revalidatePath(`/thread/${threadId}`);
}
```

#### Semantic Memory Query
```tsx
// lib/memory.ts
export async function retrieveMemory(query: string, k=3) {
  const embedding = await embed(query);
  return chromaCollection.query({ queryEmbeddings: [embedding], nResults: k });
}
```

#### Thread Rendering (Recursive)
```tsx
// components/ThreadMessage.tsx
export function ThreadMessage({ message, depth=0 }) {
  const replies = messages.filter(m => m.parentId === message.id);
  return (
    <div className={`ml-${depth * 4}`}>
      {message.content}
      {replies.map(reply => <ThreadMessage message={reply} depth={depth+1}/>)}
    </div>
  );
}
```

#### Agent Orchestration
```tsx
// lib/agents/orchestrator.ts
const flow = {
  nodes: [{ id: 'generator' }, { id: 'critic' }],
  edges: [{ from: 'generator', to: 'critic' }]
};
const results = await orchestrator.executeFlow(flow, { question });
```

#### LLM Prompt Structure
```tsx
// lib/prompts.ts
export function buildAgentPrompt(messages: Message[]) {
  return [
    { role: 'system', content: 'You are a thoughtful assistant...' },
    ...messages.map(m => ({ role: m.role, content: m.content }))
  ];
}
```

#### Local LLM Call (Ollama)
```tsx
// lib/llm.ts
export async function callLocalLLM(prompt: string) {
  const res = await fetch('http://localhost:11434/api/generate', {
    method: 'POST',
    body: JSON.stringify({ model: 'mistral', prompt, stream: false })
  });
  return (await res.json()).response;
}
```

#### Real-Time Streaming
```tsx
// app/api/chat/route.ts
export async function POST(req: Request) {
  const stream = OpenAIStream(await openai.chat.completions.create({
    stream: true,
    messages: await req.json()
  }));
  return new StreamingTextResponse(stream);
}
```

*Pro Tip*: Combine these patterns to create new capabilities:
1. Add emotion analysis to memory retrieval
2. Wrap agent flows in biometric conditionals
3. Generate journal summaries from thread clusters

---
