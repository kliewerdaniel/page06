{
  "title": "Module 2: Domain-Specific Fine-Tuning",
  "weeks": "5-7",
  "status": "draft",
  "body": {
    "raw": "# Module 2: Transformer Implementation - Week 2\n## Building GPT from Mathematical Foundations\n\n---\n\n## üéØ Module Overview\n\nThis week marks your transition from theory to implementation. You'll build a complete GPT-style transformer from scratch using PyTorch, implementing every component from the mathematical foundations up. By the end of this module, you'll have a working language model that can generate coherent text and a deep understanding of how attention mechanisms actually work.\n\n**Time Commitment:** 20-25 hours | **Difficulty:** High | **Prerequisites:** Linear algebra, Python proficiency\n\n---\n\n## üìã Learning Objectives\n\nBy completing this module, you will:\n\n### Technical Mastery\n- **Implement multi-head attention** from the mathematical formulation Q, K, V = XW_q, XW_k, XW_v\n- **Build position encodings** using sinusoidal functions and understand why they enable sequence understanding\n- **Create the complete transformer architecture** with proper residual connections and layer normalization\n- **Design efficient matrix operations** using PyTorch's tensor operations for GPU acceleration\n- **Implement text tokenization** and understand the relationship between vocabulary size and model capacity\n\n### Conceptual Understanding\n- **Grasp the attention mechanism** beyond surface-level explanations‚Äîunderstand why it solves the sequence modeling problem\n- **Recognize architectural choices** in transformer design and their impact on model performance\n- **Debug neural networks** using gradient flow analysis and activation visualization\n- **Optimize memory usage** through batching strategies and gradient checkpointing\n\n### Professional Skills\n- **Write modular, testable code** following software engineering best practices\n- **Create comprehensive documentation** explaining complex mathematical concepts\n- **Profile and optimize** PyTorch code for training efficiency\n- **Use version control** effectively for iterative model development\n\n---\n\n## üèóÔ∏è Architecture Overview\n\n### The Complete Transformer Stack\n\n```\nInput Text ‚Üí Tokenization ‚Üí Embedding ‚Üí Position Encoding\n                                ‚Üì\n    Multi-Head Attention ‚Üí Add & Norm ‚Üí Feed Forward\n                                ‚Üì\n    Multi-Head Attention ‚Üí Add & Norm ‚Üí Feed Forward\n                                ‚Üì\n                        ... (repeat N layers)\n                                ‚Üì\n            Layer Norm ‚Üí Linear Head ‚Üí Softmax ‚Üí Output Probabilities\n```\n\n### Key Components You'll Build\n\n1. **Token Embeddings** - Convert discrete tokens to continuous vectors\n2. **Positional Encodings** - Inject sequence position information\n3. **Multi-Head Attention** - The core mechanism for sequence modeling\n4. **Feed-Forward Networks** - Position-wise transformations\n5. **Layer Normalization** - Stabilize training dynamics\n6. **Output Head** - Convert hidden states to token probabilities\n\n---\n\n## üìä Week Structure\n\n### Day 1-2: Mathematical Foundations & Setup\n**Goal:** Understand the theory and set up your development environment\n\n#### Mathematical Deep Dive\n- **Attention Mechanism Derivation**\n  - Start with the intuition: \"What should the model pay attention to?\"\n  - Derive the scaled dot-product attention formula: Attention(Q,K,V) = softmax(QK^T/‚àöd_k)V\n  - Understand why scaling by ‚àöd_k prevents vanishing gradients\n  - Work through concrete examples with small matrices\n\n- **Multi-Head Attention Mathematics**\n  - Why multiple heads? Understanding different types of linguistic relationships\n  - Mathematical formulation: MultiHead(Q,K,V) = Concat(head_1, ..., head_h)W^O\n  - Parameter counting: understanding the computational complexity\n\n#### Environment Setup\n```bash\n# Create project structure\nmkdir transformer-from-scratch\ncd transformer-from-scratch\ngit init\n\n# Set up Python environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install dependencies\npip install torch torchvision numpy matplotlib wandb tensorboard pytest black isort\n\n# Create directory structure\nmkdir src tests data notebooks checkpoints\ntouch src/__init__.py tests/__init__.py\n```\n\n#### Development Standards\n- **Code Quality:** Use Black for formatting, isort for imports\n- **Testing:** Write unit tests for each component\n- **Logging:** Use Python logging for debugging and monitoring\n- **Version Control:** Commit frequently with descriptive messages\n\n### Day 3-4: Core Components Implementation\n\n#### Token Embeddings & Positional Encoding\n\n**Implementation Focus:**\n```python\nclass TokenEmbedding(nn.Module):\n    def __init__(self, vocab_size: int, d_model: int):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        self.d_model = d_model\n    \n    def forward(self, x):\n        # Why do we scale by sqrt(d_model)?\n        return self.embedding(x) * math.sqrt(self.d_model)\n```\n\n**Key Learning Points:**\n- Why embedding dimensions matter for model capacity\n- The relationship between vocabulary size and computational cost\n- How scaling affects gradient flow during training\n\n#### Positional Encoding Deep Dive\n```python\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model: int, max_len: int = 5000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1).float()\n        \n        # Why sinusoidal? Mathematical derivation and intuition\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                           -(math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        self.register_buffer('pe', pe.unsqueeze(0))\n```\n\n**Understanding Questions to Answer:**\n- Why not learnable position embeddings?\n- How do sinusoidal encodings enable length generalization?\n- What happens to position information in deeper layers?\n\n#### Multi-Head Attention Implementation\n\n**Core Implementation:**\n```python\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n        super().__init__()\n        assert d_model % n_heads == 0\n        \n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n        \n        # Linear transformations for Q, K, V\n        self.w_q = nn.Linear(d_model, d_model, bias=False)\n        self.w_k = nn.Linear(d_model, d_model, bias=False)\n        self.w_v = nn.Linear(d_model, d_model, bias=False)\n        self.w_o = nn.Linear(d_model, d_model)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        # The heart of the transformer\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        \n        if mask is not None:\n            scores.masked_fill_(mask == 0, -1e9)\n        \n        attention_weights = F.softmax(scores, dim=-1)\n        attention_weights = self.dropout(attention_weights)\n        \n        return torch.matmul(attention_weights, V), attention_weights\n```\n\n**Critical Implementation Details:**\n- **Masking:** Implementing causal masking for autoregressive generation\n- **Efficient Computation:** Batched matrix operations for multiple heads\n- **Gradient Flow:** Understanding how attention weights affect backpropagation\n\n### Day 5-6: Complete Architecture & Training Setup\n\n#### Transformer Block Assembly\n```python\nclass TransformerBlock(nn.Module):\n    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float):\n        super().__init__()\n        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x, mask=None):\n        # Pre-normalization vs post-normalization - why does it matter?\n        attn_output, _ = self.attention(self.norm1(x), self.norm1(x), self.norm1(x), mask)\n        x = x + self.dropout(attn_output)\n        \n        ff_output = self.feed_forward(self.norm2(x))\n        x = x + self.dropout(ff_output)\n        \n        return x\n```\n\n#### Complete GPT Model\n```python\nclass GPT(nn.Module):\n    def __init__(self, vocab_size: int, d_model: int, n_heads: int, \n                 n_layers: int, d_ff: int, max_len: int, dropout: float = 0.1):\n        super().__init__()\n        self.token_embedding = TokenEmbedding(vocab_size, d_model)\n        self.pos_encoding = PositionalEncoding(d_model, max_len)\n        \n        self.transformer_blocks = nn.ModuleList([\n            TransformerBlock(d_model, n_heads, d_ff, dropout)\n            for _ in range(n_layers)\n        ])\n        \n        self.ln_f = nn.LayerNorm(d_model)\n        self.head = nn.Linear(d_model, vocab_size, bias=False)\n        \n        # Weight tying - why tie input and output embeddings?\n        self.head.weight = self.token_embedding.embedding.weight\n        \n        self.apply(self._init_weights)\n    \n    def _init_weights(self, module):\n        # Proper weight initialization is crucial\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n```\n\n#### Training Infrastructure\n```python\nclass GPTTrainer:\n    def __init__(self, model, train_loader, val_loader, config):\n        self.model = model\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.config = config\n        \n        # Optimizer with proper hyperparameters\n        self.optimizer = torch.optim.AdamW(\n            model.parameters(),\n            lr=config.learning_rate,\n            betas=(0.9, 0.95),\n            weight_decay=config.weight_decay\n        )\n        \n        # Learning rate scheduling\n        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            self.optimizer, T_max=config.max_steps\n        )\n        \n    def train_step(self, batch):\n        self.model.train()\n        self.optimizer.zero_grad()\n        \n        input_ids, targets = batch\n        logits = self.model(input_ids)\n        \n        # Compute loss - why cross-entropy for language modeling?\n        loss = F.cross_entropy(\n            logits.view(-1, logits.size(-1)),\n            targets.view(-1),\n            ignore_index=-1\n        )\n        \n        loss.backward()\n        \n        # Gradient clipping - essential for transformer training\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n        \n        self.optimizer.step()\n        self.scheduler.step()\n        \n        return loss.item()\n```\n\n### Day 7: Testing, Debugging & Text Generation\n\n#### Comprehensive Testing Strategy\n```python\ndef test_attention_mechanism():\n    \"\"\"Test that attention weights sum to 1 and have correct shape\"\"\"\n    d_model, n_heads, seq_len, batch_size = 512, 8, 100, 32\n    \n    attention = MultiHeadAttention(d_model, n_heads)\n    x = torch.randn(batch_size, seq_len, d_model)\n    \n    output, weights = attention(x, x, x)\n    \n    # Shape assertions\n    assert output.shape == x.shape\n    assert weights.shape == (batch_size, n_heads, seq_len, seq_len)\n    \n    # Attention weights should sum to 1\n    assert torch.allclose(weights.sum(dim=-1), torch.ones_like(weights.sum(dim=-1)))\n\ndef test_gradient_flow():\n    \"\"\"Ensure gradients flow through all components\"\"\"\n    model = GPT(vocab_size=1000, d_model=256, n_heads=8, n_layers=6, d_ff=1024, max_len=512)\n    \n    input_ids = torch.randint(0, 1000, (2, 50))\n    targets = torch.randint(0, 1000, (2, 50))\n    \n    logits = model(input_ids)\n    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n    loss.backward()\n    \n    # Check that all parameters have gradients\n    for name, param in model.named_parameters():\n        assert param.grad is not None, f\"No gradient for {name}\"\n```\n\n#### Text Generation Implementation\n```python\ndef generate_text(model, tokenizer, prompt, max_length=100, temperature=0.8, top_k=50):\n    \"\"\"Generate text using the trained model\"\"\"\n    model.eval()\n    \n    # Tokenize prompt\n    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n    \n    with torch.no_grad():\n        for _ in range(max_length):\n            # Forward pass\n            logits = model(input_ids)\n            \n            # Get next token probabilities\n            next_token_logits = logits[0, -1, :] / temperature\n            \n            # Apply top-k filtering\n            if top_k > 0:\n                top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n                next_token_logits = torch.full_like(next_token_logits, float('-inf'))\n                next_token_logits[top_k_indices] = top_k_logits\n            \n            # Sample next token\n            probs = F.softmax(next_token_logits, dim=-1)\n            next_token = torch.multinomial(probs, 1)\n            \n            # Append to sequence\n            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=-1)\n            \n            # Stop at end token\n            if next_token.item() == tokenizer.eos_token_id:\n                break\n    \n    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n```\n\n---\n\n## üõ†Ô∏è Hands-On Exercises\n\n### Exercise 1: Attention Visualization\nCreate visualizations showing what your attention heads are learning:\n\n```python\ndef visualize_attention_patterns(model, text, layer_idx=0, head_idx=0):\n    \"\"\"Create heatmap showing attention patterns\"\"\"\n    model.eval()\n    \n    # Get attention weights from specific layer/head\n    with torch.no_grad():\n        # Hook to capture attention weights\n        attention_weights = []\n        \n        def hook_fn(module, input, output):\n            attention_weights.append(output[1])  # Attention weights\n        \n        hook = model.transformer_blocks[layer_idx].attention.register_forward_hook(hook_fn)\n        \n        # Forward pass\n        input_ids = tokenizer.encode(text, return_tensors='pt')\n        _ = model(input_ids)\n        \n        hook.remove()\n    \n    # Create heatmap\n    weights = attention_weights[0][0, head_idx].cpu().numpy()\n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n    \n    plt.figure(figsize=(12, 8))\n    sns.heatmap(weights, xticklabels=tokens, yticklabels=tokens, cmap='Blues')\n    plt.title(f'Attention Pattern - Layer {layer_idx}, Head {head_idx}')\n    plt.show()\n```\n\n### Exercise 2: Architecture Ablation Study\nSystematically remove components to understand their importance:\n\n```python\ndef ablation_study():\n    \"\"\"Compare model performance with different architectural choices\"\"\"\n    configs = [\n        {'n_heads': 1, 'name': 'single_head'},\n        {'n_heads': 8, 'name': 'multi_head'},\n        {'n_layers': 3, 'name': 'shallow'},\n        {'n_layers': 12, 'name': 'deep'},\n        {'dropout': 0.0, 'name': 'no_dropout'},\n        {'dropout': 0.1, 'name': 'with_dropout'},\n    ]\n    \n    results = {}\n    for config in configs:\n        model = GPT(**config)\n        trainer = GPTTrainer(model, train_loader, val_loader, config)\n        \n        # Train for fixed number of steps\n        for step in range(1000):\n            loss = trainer.train_step(next(iter(train_loader)))\n        \n        # Evaluate\n        val_loss = evaluate_model(model, val_loader)\n        results[config['name']] = val_loss\n    \n    return results\n```\n\n### Exercise 3: Memory Optimization\nImplement techniques to reduce memory usage:\n\n```python\nclass MemoryEfficientGPT(GPT):\n    \"\"\"GPT with memory optimization techniques\"\"\"\n    \n    def __init__(self, *args, use_checkpointing=True, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.use_checkpointing = use_checkpointing\n    \n    def forward(self, x, mask=None):\n        x = self.token_embedding(x) + self.pos_encoding(x)\n        \n        for block in self.transformer_blocks:\n            if self.use_checkpointing and self.training:\n                # Use gradient checkpointing to save memory\n                x = torch.utils.checkpoint.checkpoint(block, x, mask)\n            else:\n                x = block(x, mask)\n        \n        x = self.ln_f(x)\n        return self.head(x)\n```\n\n---\n\n## üìà Performance Benchmarks\n\n### Training Targets\nBy the end of the week, your model should achieve:\n\n- **Perplexity:** < 150 on validation set (WikiText-2)\n- **Training Speed:** > 1000 tokens/second on single GPU\n- **Memory Usage:** < 8GB for 50M parameter model\n- **Generation Quality:** Coherent text for 2-3 sentences\n\n### Optimization Checklist\n- [ ] Mixed precision training (FP16)\n- [ ] Gradient accumulation for larger effective batch sizes\n- [ ] Efficient data loading with multiple workers\n- [ ] Proper learning rate scheduling\n- [ ] Gradient clipping to prevent instability\n\n---\n\n## üìù Deliverables\n\n### 1. Working Implementation (40 points)\n**File:** `src/transformer.py`\n\nComplete, documented implementation including:\n- All transformer components with proper shapes and operations\n- Training loop with logging and checkpointing\n- Text generation function with temperature and top-k sampling\n- Comprehensive unit tests with >90% coverage\n\n**Evaluation Criteria:**\n- Code runs without errors and produces reasonable outputs\n- Proper PyTorch best practices (device handling, gradient management)\n- Modular design with clear separation of concerns\n- Performance meets benchmark targets\n\n### 2. Technical Documentation (35 points)\n**File:** `README.md` + `docs/architecture.md`\n\nComprehensive documentation including:\n- Mathematical derivation of key components (attention, positional encoding)\n- Architecture diagrams showing data flow\n- Training procedure and hyperparameter choices\n- Performance analysis and optimization techniques\n\n**Required Sections:**\n1. **Mathematical Foundation** - Derive attention mechanism step-by-step\n2. **Architecture Overview** - Visual diagrams with component explanations\n3. **Implementation Details** - Key design decisions and their rationale\n4. **Training Process** - Loss curves, convergence analysis, hyperparameter sensitivity\n5. **Performance Analysis** - Speed benchmarks, memory usage, scaling behavior\n\n### 3. Experimental Analysis (25 points)\n**File:** `notebooks/transformer_analysis.ipynb`\n\nJupyter notebook containing:\n- Attention pattern visualizations for different heads and layers\n- Ablation study results comparing architectural choices\n- Loss curve analysis and convergence behavior\n- Text generation examples with different sampling strategies\n- Performance profiling and optimization analysis\n\n**Analysis Requirements:**\n- Statistical significance testing for architectural comparisons\n- Qualitative analysis of generated text quality\n- Quantitative metrics (perplexity, BLEU scores where applicable)\n- Memory and computational complexity analysis\n\n---\n\n## üîç Common Pitfalls & Debugging Guide\n\n### Implementation Issues\n\n**Problem:** Attention weights don't sum to 1\n```python\n# Wrong: Softmax over wrong dimension\nattention_weights = F.softmax(scores, dim=-2)  # ‚ùå\n\n# Correct: Softmax over key dimension\nattention_weights = F.softmax(scores, dim=-1)  # ‚úÖ\n```\n\n**Problem:** Gradient explosion during training\n```python\n# Add gradient clipping\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n# Use proper weight initialization\ndef _init_weights(self, module):\n    if isinstance(module, nn.Linear):\n        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n```\n\n**Problem:** Slow training speed\n```python\n# Enable mixed precision training\nscaler = torch.cuda.amp.GradScaler()\n\nwith torch.cuda.amp.autocast():\n    logits = model(input_ids)\n    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n\nscaler.scale(loss).backward()\nscaler.step(optimizer)\nscaler.update()\n```\n\n### Conceptual Issues\n\n**Q: Why do we scale attention scores by ‚àöd_k?**\nA: Without scaling, the dot products grow large as d_k increases, pushing the softmax into regions with extremely small gradients, making training unstable.\n\n**Q: Why use sinusoidal positional encodings instead of learned embeddings?**\nA: Sinusoidal encodings allow the model to extrapolate to sequence lengths longer than those seen during training, while learned embeddings are fixed to the maximum training length.\n\n**Q: What's the difference between pre-norm and post-norm architectures?**\nA: Pre-norm (used in modern transformers) applies layer normalization before the sub-layers, leading to more stable training and better gradient flow.\n\n---\n\n## üìö Extended Learning Resources\n\n### Essential Papers\n1. **\"Attention Is All You Need\"** (Vaswani et al., 2017) - The original transformer paper\n2. **\"Language Models are Unsupervised Multitask Learners\"** (Radford et al., 2019) - GPT-2 architecture and scaling insights\n3. **\"On Layer Normalization in the Transformer Architecture\"** (Xiong et al., 2020) - Pre-norm vs post-norm analysis\n\n### Technical Deep Dives\n- **The Illustrated Transformer** by Jay Alammar - Visual explanations of transformer mechanics\n- **The Annotated Transformer** by Harvard NLP - Code walkthrough with mathematical explanations\n- **Transformer Circuits Thread** by Anthropic - Understanding what transformers learn\n\n### Implementation References\n- **nanoGPT** by Andrej Karpathy - Minimal, educational GPT implementation\n- **Transformers Library** by Hugging Face - Production-ready implementations\n- **Fairseq** by Facebook AI - Research-oriented transformer implementations\n\n---\n\n## üéØ Success Metrics & Next Steps\n\n### Week 2 Completion Criteria\n- [ ] All unit tests pass with >90% code coverage\n- [ ] Model achieves target perplexity on validation set\n- [ ] Documentation includes mathematical derivations and architecture diagrams\n- [ ] Generated text demonstrates basic coherence and grammar\n- [ ] Code follows professional standards (formatting, documentation, version control)\n\n### Preparation for Week 3\n- **Review fine-tuning literature** - Understand transfer learning for language models\n- **Explore parameter-efficient methods** - LoRA, adapters, and prompt tuning\n- **Study evaluation metrics** - BLEU, ROUGE, perplexity, and human evaluation\n- **Familiarize with domain adaptation** - How to specialize models for specific tasks\n\n### Career Development Notes\nThis implementation demonstrates several key skills employers look for:\n- **Deep technical knowledge** - Understanding transformers at the mathematical level\n- **Implementation ability** - Building complex systems from scratch\n- **Optimization skills** - Making models efficient and scalable\n- **Documentation** - Explaining complex concepts clearly\n- **Testing** - Ensuring code reliability and correctness\n\nAdd this project to your portfolio with emphasis on the mathematical depth and implementation quality. The ability to build transformers from scratch sets you apart from developers who only use pre-built models.\n\n---\n\n*Next week, we'll take your transformer implementation and fine-tune it for specific domains using parameter-efficient techniques like LoRA. You'll learn how to adapt pre-trained models for specialized tasks while maintaining computational efficiency.*",
    "code": "var Component=(()=>{var le=Object.create;var _=Object.defineProperty;var ae=Object.getOwnPropertyDescriptor;var se=Object.getOwnPropertyNames;var ue=Object.getPrototypeOf,te=Object.prototype.hasOwnProperty;var x=(r,e)=>()=>(e||r((e={exports:{}}).exports,e),e.exports),me=(r,e)=>{for(var u in e)_(r,u,{get:e[u],enumerable:!0})},C=(r,e,u,b)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let t of se(e))!te.call(r,t)&&t!==u&&_(r,t,{get:()=>e[t],enumerable:!(b=ae(e,t))||b.enumerable});return r};var oe=(r,e,u)=>(u=r!=null?le(ue(r)):{},C(e||!r||!r.__esModule?_(u,\"default\",{value:r,enumerable:!0}):u,r)),ce=r=>C(_({},\"__esModule\",{value:!0}),r);var O=x((he,R)=>{R.exports=React});var M=x(y=>{\"use strict\";(function(){function r(i){if(i==null)return null;if(typeof i==\"function\")return i.$$typeof===ee?null:i.displayName||i.name||null;if(typeof i==\"string\")return i;switch(i){case g:return\"Fragment\";case Y:return\"Profiler\";case V:return\"StrictMode\";case X:return\"Suspense\";case $:return\"SuspenseList\";case Z:return\"Activity\"}if(typeof i==\"object\")switch(typeof i.tag==\"number\"&&console.error(\"Received an unexpected object in getComponentNameFromType(). This is likely a bug in React. Please file an issue.\"),i.$$typeof){case H:return\"Portal\";case Q:return(i.displayName||\"Context\")+\".Provider\";case B:return(i._context.displayName||\"Context\")+\".Consumer\";case K:var d=i.render;return i=i.displayName,i||(i=d.displayName||d.name||\"\",i=i!==\"\"?\"ForwardRef(\"+i+\")\":\"ForwardRef\"),i;case J:return d=i.displayName||null,d!==null?d:r(i.type)||\"Memo\";case v:d=i._payload,i=i._init;try{return r(i(d))}catch{}}return null}function e(i){return\"\"+i}function u(i){try{e(i);var d=!1}catch{d=!0}if(d){d=console;var l=d.error,s=typeof Symbol==\"function\"&&Symbol.toStringTag&&i[Symbol.toStringTag]||i.constructor.name||\"Object\";return l.call(d,\"The provided key is an unsupported type %s. This value must be coerced to a string before using it here.\",s),e(i)}}function b(i){if(i===g)return\"<>\";if(typeof i==\"object\"&&i!==null&&i.$$typeof===v)return\"<...>\";try{var d=r(i);return d?\"<\"+d+\">\":\"<...>\"}catch{return\"<...>\"}}function t(){var i=T.A;return i===null?null:i.getOwner()}function k(){return Error(\"react-stack-top-frame\")}function j(i){if(E.call(i,\"key\")){var d=Object.getOwnPropertyDescriptor(i,\"key\").get;if(d&&d.isReactWarning)return!1}return i.key!==void 0}function I(i,d){function l(){A||(A=!0,console.error(\"%s: `key` is not a prop. Trying to access it will result in `undefined` being returned. If you need to access the same value within the child component, you should pass it as a different prop. (https://react.dev/link/special-props)\",d))}l.isReactWarning=!0,Object.defineProperty(i,\"key\",{get:l,configurable:!0})}function G(){var i=r(this.type);return z[i]||(z[i]=!0,console.error(\"Accessing element.ref was removed in React 19. ref is now a regular prop. It will be removed from the JSX Element type in a future release.\")),i=this.props.ref,i!==void 0?i:null}function W(i,d,l,s,m,o,c,h){return l=o.ref,i={$$typeof:U,type:i,key:d,props:o,_owner:m},(l!==void 0?l:null)!==null?Object.defineProperty(i,\"ref\",{enumerable:!1,get:G}):Object.defineProperty(i,\"ref\",{enumerable:!1,value:null}),i._store={},Object.defineProperty(i._store,\"validated\",{configurable:!1,enumerable:!1,writable:!0,value:0}),Object.defineProperty(i,\"_debugInfo\",{configurable:!1,enumerable:!1,writable:!0,value:null}),Object.defineProperty(i,\"_debugStack\",{configurable:!1,enumerable:!1,writable:!0,value:c}),Object.defineProperty(i,\"_debugTask\",{configurable:!1,enumerable:!1,writable:!0,value:h}),Object.freeze&&(Object.freeze(i.props),Object.freeze(i)),i}function q(i,d,l,s,m,o,c,h){var a=d.children;if(a!==void 0)if(s)if(ne(a)){for(s=0;s<a.length;s++)w(a[s]);Object.freeze&&Object.freeze(a)}else console.error(\"React.jsx: Static children should always be an array. You are likely explicitly calling React.jsxs or React.jsxDEV. Use the Babel transform instead.\");else w(a);if(E.call(d,\"key\")){a=r(i);var f=Object.keys(d).filter(function(re){return re!==\"key\"});s=0<f.length?\"{key: someKey, \"+f.join(\": ..., \")+\": ...}\":\"{key: someKey}\",S[a+s]||(f=0<f.length?\"{\"+f.join(\": ..., \")+\": ...}\":\"{}\",console.error(`A props object containing a \"key\" prop is being spread into JSX:\n  let props = %s;\n  <%s {...props} />\nReact keys must be passed directly to JSX without using spread:\n  let props = %s;\n  <%s key={someKey} {...props} />`,s,a,f,a),S[a+s]=!0)}if(a=null,l!==void 0&&(u(l),a=\"\"+l),j(d)&&(u(d.key),a=\"\"+d.key),\"key\"in d){l={};for(var N in d)N!==\"key\"&&(l[N]=d[N])}else l=d;return a&&I(l,typeof i==\"function\"?i.displayName||i.name||\"Unknown\":i),W(i,a,o,m,t(),l,c,h)}function w(i){typeof i==\"object\"&&i!==null&&i.$$typeof===U&&i._store&&(i._store.validated=1)}var p=O(),U=Symbol.for(\"react.transitional.element\"),H=Symbol.for(\"react.portal\"),g=Symbol.for(\"react.fragment\"),V=Symbol.for(\"react.strict_mode\"),Y=Symbol.for(\"react.profiler\");Symbol.for(\"react.provider\");var B=Symbol.for(\"react.consumer\"),Q=Symbol.for(\"react.context\"),K=Symbol.for(\"react.forward_ref\"),X=Symbol.for(\"react.suspense\"),$=Symbol.for(\"react.suspense_list\"),J=Symbol.for(\"react.memo\"),v=Symbol.for(\"react.lazy\"),Z=Symbol.for(\"react.activity\"),ee=Symbol.for(\"react.client.reference\"),T=p.__CLIENT_INTERNALS_DO_NOT_USE_OR_WARN_USERS_THEY_CANNOT_UPGRADE,E=Object.prototype.hasOwnProperty,ne=Array.isArray,P=console.createTask?console.createTask:function(){return null};p={\"react-stack-bottom-frame\":function(i){return i()}};var A,z={},ie=p[\"react-stack-bottom-frame\"].bind(p,k)(),de=P(b(k)),S={};y.Fragment=g,y.jsxDEV=function(i,d,l,s,m,o){var c=1e4>T.recentlyCreatedOwnerStacks++;return q(i,d,l,s,m,o,c?Error(\"react-stack-top-frame\"):ie,c?P(b(i)):de)}})()});var F=x((xe,D)=>{\"use strict\";D.exports=M()});var _e={};me(_e,{default:()=>pe,frontmatter:()=>fe});var n=oe(F()),fe={title:\"Module 2: Domain-Specific Fine-Tuning\",weeks:\"5-7\",status:\"draft\",type:\"ModuleDoc\"};function L(r){let e=Object.assign({h1:\"h1\",h2:\"h2\",hr:\"hr\",p:\"p\",strong:\"strong\",h3:\"h3\",ul:\"ul\",li:\"li\",pre:\"pre\",code:\"code\",ol:\"ol\",h4:\"h4\",em:\"em\"},r.components);return(0,n.jsxDEV)(n.Fragment,{children:[(0,n.jsxDEV)(e.h1,{children:\"Module 2: Transformer Implementation - Week 2\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:7,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h2,{children:\"Building GPT from Mathematical Foundations\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:8,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.hr,{},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:10,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h2,{children:\"\\u{1F3AF} Module Overview\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:12,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.p,{children:\"This week marks your transition from theory to implementation. You'll build a complete GPT-style transformer from scratch using PyTorch, implementing every component from the mathematical foundations up. By the end of this module, you'll have a working language model that can generate coherent text and a deep understanding of how attention mechanisms actually work.\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:14,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.p,{children:[(0,n.jsxDEV)(e.strong,{children:\"Time Commitment:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:16,columnNumber:1},this),\" 20-25 hours | \",(0,n.jsxDEV)(e.strong,{children:\"Difficulty:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:16,columnNumber:36},this),\" High | \",(0,n.jsxDEV)(e.strong,{children:\"Prerequisites:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:16,columnNumber:59},this),\" Linear algebra, Python proficiency\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:16,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.hr,{},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:18,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h2,{children:\"\\u{1F4CB} Learning Objectives\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:20,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.p,{children:\"By completing this module, you will:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:22,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h3,{children:\"Technical Mastery\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:24,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.ul,{children:[`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Implement multi-head attention\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:25,columnNumber:3},this),\" from the mathematical formulation Q, K, V = XW_q, XW_k, XW_v\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:25,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Build position encodings\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:26,columnNumber:3},this),\" using sinusoidal functions and understand why they enable sequence understanding\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:26,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Create the complete transformer architecture\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:27,columnNumber:3},this),\" with proper residual connections and layer normalization\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:27,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Design efficient matrix operations\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:28,columnNumber:3},this),\" using PyTorch's tensor operations for GPU acceleration\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:28,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Implement text tokenization\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:29,columnNumber:3},this),\" and understand the relationship between vocabulary size and model capacity\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:29,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:25,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h3,{children:\"Conceptual Understanding\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:31,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.ul,{children:[`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Grasp the attention mechanism\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:32,columnNumber:3},this),\" beyond surface-level explanations\\u2014understand why it solves the sequence modeling problem\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:32,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Recognize architectural choices\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:33,columnNumber:3},this),\" in transformer design and their impact on model performance\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:33,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Debug neural networks\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:34,columnNumber:3},this),\" using gradient flow analysis and activation visualization\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:34,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Optimize memory usage\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:35,columnNumber:3},this),\" through batching strategies and gradient checkpointing\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:35,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:32,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h3,{children:\"Professional Skills\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:37,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.ul,{children:[`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Write modular, testable code\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:38,columnNumber:3},this),\" following software engineering best practices\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:38,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Create comprehensive documentation\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:39,columnNumber:3},this),\" explaining complex mathematical concepts\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:39,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Profile and optimize\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:40,columnNumber:3},this),\" PyTorch code for training efficiency\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:40,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Use version control\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:41,columnNumber:3},this),\" effectively for iterative model development\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:41,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:38,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.hr,{},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:43,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h2,{children:\"\\u{1F3D7}\\uFE0F Architecture Overview\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:45,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h3,{children:\"The Complete Transformer Stack\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:47,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.pre,{children:(0,n.jsxDEV)(e.code,{children:`Input Text \\u2192 Tokenization \\u2192 Embedding \\u2192 Position Encoding\n                                \\u2193\n    Multi-Head Attention \\u2192 Add & Norm \\u2192 Feed Forward\n                                \\u2193\n    Multi-Head Attention \\u2192 Add & Norm \\u2192 Feed Forward\n                                \\u2193\n                        ... (repeat N layers)\n                                \\u2193\n            Layer Norm \\u2192 Linear Head \\u2192 Softmax \\u2192 Output Probabilities\n`},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:49,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:49,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h3,{children:\"Key Components You'll Build\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:61,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.ol,{children:[`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Token Embeddings\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:63,columnNumber:4},this),\" - Convert discrete tokens to continuous vectors\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:63,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Positional Encodings\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:64,columnNumber:4},this),\" - Inject sequence position information\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:64,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Multi-Head Attention\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:65,columnNumber:4},this),\" - The core mechanism for sequence modeling\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:65,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Feed-Forward Networks\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:66,columnNumber:4},this),\" - Position-wise transformations\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:66,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Layer Normalization\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:67,columnNumber:4},this),\" - Stabilize training dynamics\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:67,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Output Head\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:68,columnNumber:4},this),\" - Convert hidden states to token probabilities\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:68,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:63,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.hr,{},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:70,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h2,{children:\"\\u{1F4CA} Week Structure\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:72,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h3,{children:\"Day 1-2: Mathematical Foundations & Setup\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:74,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.p,{children:[(0,n.jsxDEV)(e.strong,{children:\"Goal:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:75,columnNumber:1},this),\" Understand the theory and set up your development environment\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:75,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h4,{children:\"Mathematical Deep Dive\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:77,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.ul,{children:[`\n`,(0,n.jsxDEV)(e.li,{children:[`\n`,(0,n.jsxDEV)(e.p,{children:(0,n.jsxDEV)(e.strong,{children:\"Attention Mechanism Derivation\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:78,columnNumber:3},this)},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:78,columnNumber:3},this),`\n`,(0,n.jsxDEV)(e.ul,{children:[`\n`,(0,n.jsxDEV)(e.li,{children:'Start with the intuition: \"What should the model pay attention to?\"'},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:79,columnNumber:3},this),`\n`,(0,n.jsxDEV)(e.li,{children:\"Derive the scaled dot-product attention formula: Attention(Q,K,V) = softmax(QK^T/\\u221Ad_k)V\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:80,columnNumber:3},this),`\n`,(0,n.jsxDEV)(e.li,{children:\"Understand why scaling by \\u221Ad_k prevents vanishing gradients\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:81,columnNumber:3},this),`\n`,(0,n.jsxDEV)(e.li,{children:\"Work through concrete examples with small matrices\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:82,columnNumber:3},this),`\n`]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:79,columnNumber:3},this),`\n`]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:78,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[`\n`,(0,n.jsxDEV)(e.p,{children:(0,n.jsxDEV)(e.strong,{children:\"Multi-Head Attention Mathematics\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:84,columnNumber:3},this)},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:84,columnNumber:3},this),`\n`,(0,n.jsxDEV)(e.ul,{children:[`\n`,(0,n.jsxDEV)(e.li,{children:\"Why multiple heads? Understanding different types of linguistic relationships\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:85,columnNumber:3},this),`\n`,(0,n.jsxDEV)(e.li,{children:\"Mathematical formulation: MultiHead(Q,K,V) = Concat(head_1, ..., head_h)W^O\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:86,columnNumber:3},this),`\n`,(0,n.jsxDEV)(e.li,{children:\"Parameter counting: understanding the computational complexity\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:87,columnNumber:3},this),`\n`]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:85,columnNumber:3},this),`\n`]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:84,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:78,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h4,{children:\"Environment Setup\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:89,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.pre,{children:(0,n.jsxDEV)(e.code,{className:\"language-bash\",children:`# Create project structure\nmkdir transformer-from-scratch\ncd transformer-from-scratch\ngit init\n\n# Set up Python environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\\\Scripts\\\\activate\n\n# Install dependencies\npip install torch torchvision numpy matplotlib wandb tensorboard pytest black isort\n\n# Create directory structure\nmkdir src tests data notebooks checkpoints\ntouch src/__init__.py tests/__init__.py\n`},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:90,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:90,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h4,{children:\"Development Standards\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:108,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.ul,{children:[`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Code Quality:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:109,columnNumber:3},this),\" Use Black for formatting, isort for imports\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:109,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Testing:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:110,columnNumber:3},this),\" Write unit tests for each component\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:110,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Logging:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:111,columnNumber:3},this),\" Use Python logging for debugging and monitoring\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:111,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Version Control:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:112,columnNumber:3},this),\" Commit frequently with descriptive messages\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:112,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:109,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h3,{children:\"Day 3-4: Core Components Implementation\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:114,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h4,{children:\"Token Embeddings & Positional Encoding\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:116,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.p,{children:(0,n.jsxDEV)(e.strong,{children:\"Implementation Focus:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:118,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:118,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.pre,{children:(0,n.jsxDEV)(e.code,{className:\"language-python\",children:`class TokenEmbedding(nn.Module):\n    def __init__(self, vocab_size: int, d_model: int):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        self.d_model = d_model\n    \n    def forward(self, x):\n        # Why do we scale by sqrt(d_model)?\n        return self.embedding(x) * math.sqrt(self.d_model)\n`},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:119,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:119,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.p,{children:(0,n.jsxDEV)(e.strong,{children:\"Key Learning Points:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:131,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:131,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.ul,{children:[`\n`,(0,n.jsxDEV)(e.li,{children:\"Why embedding dimensions matter for model capacity\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:132,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:\"The relationship between vocabulary size and computational cost\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:133,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:\"How scaling affects gradient flow during training\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:134,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:132,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h4,{children:\"Positional Encoding Deep Dive\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:136,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.pre,{children:(0,n.jsxDEV)(e.code,{className:\"language-python\",children:`class PositionalEncoding(nn.Module):\n    def __init__(self, d_model: int, max_len: int = 5000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1).float()\n        \n        # Why sinusoidal? Mathematical derivation and intuition\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                           -(math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        self.register_buffer('pe', pe.unsqueeze(0))\n`},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:137,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:137,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.p,{children:(0,n.jsxDEV)(e.strong,{children:\"Understanding Questions to Answer:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:153,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:153,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.ul,{children:[`\n`,(0,n.jsxDEV)(e.li,{children:\"Why not learnable position embeddings?\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:154,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:\"How do sinusoidal encodings enable length generalization?\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:155,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:\"What happens to position information in deeper layers?\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:156,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:154,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h4,{children:\"Multi-Head Attention Implementation\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:158,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.p,{children:(0,n.jsxDEV)(e.strong,{children:\"Core Implementation:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:160,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:160,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.pre,{children:(0,n.jsxDEV)(e.code,{className:\"language-python\",children:`class MultiHeadAttention(nn.Module):\n    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n        super().__init__()\n        assert d_model % n_heads == 0\n        \n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n        \n        # Linear transformations for Q, K, V\n        self.w_q = nn.Linear(d_model, d_model, bias=False)\n        self.w_k = nn.Linear(d_model, d_model, bias=False)\n        self.w_v = nn.Linear(d_model, d_model, bias=False)\n        self.w_o = nn.Linear(d_model, d_model)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        # The heart of the transformer\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        \n        if mask is not None:\n            scores.masked_fill_(mask == 0, -1e9)\n        \n        attention_weights = F.softmax(scores, dim=-1)\n        attention_weights = self.dropout(attention_weights)\n        \n        return torch.matmul(attention_weights, V), attention_weights\n`},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:161,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:161,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.p,{children:(0,n.jsxDEV)(e.strong,{children:\"Critical Implementation Details:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:192,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:192,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.ul,{children:[`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Masking:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:193,columnNumber:3},this),\" Implementing causal masking for autoregressive generation\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:193,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Efficient Computation:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:194,columnNumber:3},this),\" Batched matrix operations for multiple heads\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:194,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Gradient Flow:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:195,columnNumber:3},this),\" Understanding how attention weights affect backpropagation\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:195,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:193,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h3,{children:\"Day 5-6: Complete Architecture & Training Setup\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:197,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h4,{children:\"Transformer Block Assembly\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:199,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.pre,{children:(0,n.jsxDEV)(e.code,{className:\"language-python\",children:`class TransformerBlock(nn.Module):\n    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float):\n        super().__init__()\n        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x, mask=None):\n        # Pre-normalization vs post-normalization - why does it matter?\n        attn_output, _ = self.attention(self.norm1(x), self.norm1(x), self.norm1(x), mask)\n        x = x + self.dropout(attn_output)\n        \n        ff_output = self.feed_forward(self.norm2(x))\n        x = x + self.dropout(ff_output)\n        \n        return x\n`},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:200,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:200,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h4,{children:\"Complete GPT Model\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:221,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.pre,{children:(0,n.jsxDEV)(e.code,{className:\"language-python\",children:`class GPT(nn.Module):\n    def __init__(self, vocab_size: int, d_model: int, n_heads: int, \n                 n_layers: int, d_ff: int, max_len: int, dropout: float = 0.1):\n        super().__init__()\n        self.token_embedding = TokenEmbedding(vocab_size, d_model)\n        self.pos_encoding = PositionalEncoding(d_model, max_len)\n        \n        self.transformer_blocks = nn.ModuleList([\n            TransformerBlock(d_model, n_heads, d_ff, dropout)\n            for _ in range(n_layers)\n        ])\n        \n        self.ln_f = nn.LayerNorm(d_model)\n        self.head = nn.Linear(d_model, vocab_size, bias=False)\n        \n        # Weight tying - why tie input and output embeddings?\n        self.head.weight = self.token_embedding.embedding.weight\n        \n        self.apply(self._init_weights)\n    \n    def _init_weights(self, module):\n        # Proper weight initialization is crucial\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n`},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:222,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:222,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h4,{children:\"Training Infrastructure\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:253,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.pre,{children:(0,n.jsxDEV)(e.code,{className:\"language-python\",children:`class GPTTrainer:\n    def __init__(self, model, train_loader, val_loader, config):\n        self.model = model\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.config = config\n        \n        # Optimizer with proper hyperparameters\n        self.optimizer = torch.optim.AdamW(\n            model.parameters(),\n            lr=config.learning_rate,\n            betas=(0.9, 0.95),\n            weight_decay=config.weight_decay\n        )\n        \n        # Learning rate scheduling\n        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            self.optimizer, T_max=config.max_steps\n        )\n        \n    def train_step(self, batch):\n        self.model.train()\n        self.optimizer.zero_grad()\n        \n        input_ids, targets = batch\n        logits = self.model(input_ids)\n        \n        # Compute loss - why cross-entropy for language modeling?\n        loss = F.cross_entropy(\n            logits.view(-1, logits.size(-1)),\n            targets.view(-1),\n            ignore_index=-1\n        )\n        \n        loss.backward()\n        \n        # Gradient clipping - essential for transformer training\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n        \n        self.optimizer.step()\n        self.scheduler.step()\n        \n        return loss.item()\n`},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:254,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:254,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h3,{children:\"Day 7: Testing, Debugging & Text Generation\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:300,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h4,{children:\"Comprehensive Testing Strategy\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:302,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.pre,{children:(0,n.jsxDEV)(e.code,{className:\"language-python\",children:`def test_attention_mechanism():\n    \"\"\"Test that attention weights sum to 1 and have correct shape\"\"\"\n    d_model, n_heads, seq_len, batch_size = 512, 8, 100, 32\n    \n    attention = MultiHeadAttention(d_model, n_heads)\n    x = torch.randn(batch_size, seq_len, d_model)\n    \n    output, weights = attention(x, x, x)\n    \n    # Shape assertions\n    assert output.shape == x.shape\n    assert weights.shape == (batch_size, n_heads, seq_len, seq_len)\n    \n    # Attention weights should sum to 1\n    assert torch.allclose(weights.sum(dim=-1), torch.ones_like(weights.sum(dim=-1)))\n\ndef test_gradient_flow():\n    \"\"\"Ensure gradients flow through all components\"\"\"\n    model = GPT(vocab_size=1000, d_model=256, n_heads=8, n_layers=6, d_ff=1024, max_len=512)\n    \n    input_ids = torch.randint(0, 1000, (2, 50))\n    targets = torch.randint(0, 1000, (2, 50))\n    \n    logits = model(input_ids)\n    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n    loss.backward()\n    \n    # Check that all parameters have gradients\n    for name, param in model.named_parameters():\n        assert param.grad is not None, f\"No gradient for {name}\"\n`},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:303,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:303,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h4,{children:\"Text Generation Implementation\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:336,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.pre,{children:(0,n.jsxDEV)(e.code,{className:\"language-python\",children:`def generate_text(model, tokenizer, prompt, max_length=100, temperature=0.8, top_k=50):\n    \"\"\"Generate text using the trained model\"\"\"\n    model.eval()\n    \n    # Tokenize prompt\n    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n    \n    with torch.no_grad():\n        for _ in range(max_length):\n            # Forward pass\n            logits = model(input_ids)\n            \n            # Get next token probabilities\n            next_token_logits = logits[0, -1, :] / temperature\n            \n            # Apply top-k filtering\n            if top_k > 0:\n                top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n                next_token_logits = torch.full_like(next_token_logits, float('-inf'))\n                next_token_logits[top_k_indices] = top_k_logits\n            \n            # Sample next token\n            probs = F.softmax(next_token_logits, dim=-1)\n            next_token = torch.multinomial(probs, 1)\n            \n            # Append to sequence\n            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=-1)\n            \n            # Stop at end token\n            if next_token.item() == tokenizer.eos_token_id:\n                break\n    \n    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n`},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:337,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:337,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.hr,{},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:373,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h2,{children:\"\\u{1F6E0}\\uFE0F Hands-On Exercises\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:375,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h3,{children:\"Exercise 1: Attention Visualization\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:377,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.p,{children:\"Create visualizations showing what your attention heads are learning:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:378,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.pre,{children:(0,n.jsxDEV)(e.code,{className:\"language-python\",children:`def visualize_attention_patterns(model, text, layer_idx=0, head_idx=0):\n    \"\"\"Create heatmap showing attention patterns\"\"\"\n    model.eval()\n    \n    # Get attention weights from specific layer/head\n    with torch.no_grad():\n        # Hook to capture attention weights\n        attention_weights = []\n        \n        def hook_fn(module, input, output):\n            attention_weights.append(output[1])  # Attention weights\n        \n        hook = model.transformer_blocks[layer_idx].attention.register_forward_hook(hook_fn)\n        \n        # Forward pass\n        input_ids = tokenizer.encode(text, return_tensors='pt')\n        _ = model(input_ids)\n        \n        hook.remove()\n    \n    # Create heatmap\n    weights = attention_weights[0][0, head_idx].cpu().numpy()\n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n    \n    plt.figure(figsize=(12, 8))\n    sns.heatmap(weights, xticklabels=tokens, yticklabels=tokens, cmap='Blues')\n    plt.title(f'Attention Pattern - Layer {layer_idx}, Head {head_idx}')\n    plt.show()\n`},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:380,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:380,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h3,{children:\"Exercise 2: Architecture Ablation Study\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:411,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.p,{children:\"Systematically remove components to understand their importance:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:412,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.pre,{children:(0,n.jsxDEV)(e.code,{className:\"language-python\",children:`def ablation_study():\n    \"\"\"Compare model performance with different architectural choices\"\"\"\n    configs = [\n        {'n_heads': 1, 'name': 'single_head'},\n        {'n_heads': 8, 'name': 'multi_head'},\n        {'n_layers': 3, 'name': 'shallow'},\n        {'n_layers': 12, 'name': 'deep'},\n        {'dropout': 0.0, 'name': 'no_dropout'},\n        {'dropout': 0.1, 'name': 'with_dropout'},\n    ]\n    \n    results = {}\n    for config in configs:\n        model = GPT(**config)\n        trainer = GPTTrainer(model, train_loader, val_loader, config)\n        \n        # Train for fixed number of steps\n        for step in range(1000):\n            loss = trainer.train_step(next(iter(train_loader)))\n        \n        # Evaluate\n        val_loss = evaluate_model(model, val_loader)\n        results[config['name']] = val_loss\n    \n    return results\n`},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:414,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:414,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h3,{children:\"Exercise 3: Memory Optimization\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:442,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.p,{children:\"Implement techniques to reduce memory usage:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:443,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.pre,{children:(0,n.jsxDEV)(e.code,{className:\"language-python\",children:`class MemoryEfficientGPT(GPT):\n    \"\"\"GPT with memory optimization techniques\"\"\"\n    \n    def __init__(self, *args, use_checkpointing=True, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.use_checkpointing = use_checkpointing\n    \n    def forward(self, x, mask=None):\n        x = self.token_embedding(x) + self.pos_encoding(x)\n        \n        for block in self.transformer_blocks:\n            if self.use_checkpointing and self.training:\n                # Use gradient checkpointing to save memory\n                x = torch.utils.checkpoint.checkpoint(block, x, mask)\n            else:\n                x = block(x, mask)\n        \n        x = self.ln_f(x)\n        return self.head(x)\n`},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:445,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:445,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.hr,{},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:467,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h2,{children:\"\\u{1F4C8} Performance Benchmarks\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:469,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h3,{children:\"Training Targets\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:471,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.p,{children:\"By the end of the week, your model should achieve:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:472,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.ul,{children:[`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Perplexity:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:474,columnNumber:3},this),\" < 150 on validation set (WikiText-2)\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:474,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Training Speed:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:475,columnNumber:3},this),\" > 1000 tokens/second on single GPU\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:475,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Memory Usage:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:476,columnNumber:3},this),\" < 8GB for 50M parameter model\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:476,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Generation Quality:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:477,columnNumber:3},this),\" Coherent text for 2-3 sentences\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:477,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:474,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h3,{children:\"Optimization Checklist\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:479,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.ul,{children:[`\n`,(0,n.jsxDEV)(e.li,{children:\"[ ] Mixed precision training (FP16)\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:480,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:\"[ ] Gradient accumulation for larger effective batch sizes\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:481,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:\"[ ] Efficient data loading with multiple workers\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:482,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:\"[ ] Proper learning rate scheduling\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:483,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:\"[ ] Gradient clipping to prevent instability\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:484,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:480,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.hr,{},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:486,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h2,{children:\"\\u{1F4DD} Deliverables\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:488,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h3,{children:\"1. Working Implementation (40 points)\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:490,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.p,{children:[(0,n.jsxDEV)(e.strong,{children:\"File:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:491,columnNumber:1},this),\" \",(0,n.jsxDEV)(e.code,{children:\"src/transformer.py\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:491,columnNumber:11},this)]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:491,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.p,{children:\"Complete, documented implementation including:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:493,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.ul,{children:[`\n`,(0,n.jsxDEV)(e.li,{children:\"All transformer components with proper shapes and operations\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:494,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:\"Training loop with logging and checkpointing\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:495,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:\"Text generation function with temperature and top-k sampling\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:496,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:\"Comprehensive unit tests with >90% coverage\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:497,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:494,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.p,{children:(0,n.jsxDEV)(e.strong,{children:\"Evaluation Criteria:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:499,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:499,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.ul,{children:[`\n`,(0,n.jsxDEV)(e.li,{children:\"Code runs without errors and produces reasonable outputs\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:500,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:\"Proper PyTorch best practices (device handling, gradient management)\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:501,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:\"Modular design with clear separation of concerns\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:502,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:\"Performance meets benchmark targets\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:503,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:500,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h3,{children:\"2. Technical Documentation (35 points)\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:505,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.p,{children:[(0,n.jsxDEV)(e.strong,{children:\"File:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:506,columnNumber:1},this),\" \",(0,n.jsxDEV)(e.code,{children:\"README.md\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:506,columnNumber:11},this),\" + \",(0,n.jsxDEV)(e.code,{children:\"docs/architecture.md\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:506,columnNumber:25},this)]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:506,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.p,{children:\"Comprehensive documentation including:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:508,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.ul,{children:[`\n`,(0,n.jsxDEV)(e.li,{children:\"Mathematical derivation of key components (attention, positional encoding)\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:509,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:\"Architecture diagrams showing data flow\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:510,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:\"Training procedure and hyperparameter choices\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:511,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:\"Performance analysis and optimization techniques\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:512,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:509,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.p,{children:(0,n.jsxDEV)(e.strong,{children:\"Required Sections:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:514,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:514,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.ol,{children:[`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Mathematical Foundation\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:515,columnNumber:4},this),\" - Derive attention mechanism step-by-step\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:515,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Architecture Overview\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:516,columnNumber:4},this),\" - Visual diagrams with component explanations\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:516,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Implementation Details\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:517,columnNumber:4},this),\" - Key design decisions and their rationale\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:517,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Training Process\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:518,columnNumber:4},this),\" - Loss curves, convergence analysis, hyperparameter sensitivity\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:518,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Performance Analysis\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:519,columnNumber:4},this),\" - Speed benchmarks, memory usage, scaling behavior\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:519,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:515,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h3,{children:\"3. Experimental Analysis (25 points)\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:521,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.p,{children:[(0,n.jsxDEV)(e.strong,{children:\"File:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:522,columnNumber:1},this),\" \",(0,n.jsxDEV)(e.code,{children:\"notebooks/transformer_analysis.ipynb\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:522,columnNumber:11},this)]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:522,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.p,{children:\"Jupyter notebook containing:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:524,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.ul,{children:[`\n`,(0,n.jsxDEV)(e.li,{children:\"Attention pattern visualizations for different heads and layers\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:525,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:\"Ablation study results comparing architectural choices\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:526,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:\"Loss curve analysis and convergence behavior\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:527,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:\"Text generation examples with different sampling strategies\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:528,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:\"Performance profiling and optimization analysis\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:529,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:525,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.p,{children:(0,n.jsxDEV)(e.strong,{children:\"Analysis Requirements:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:531,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:531,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.ul,{children:[`\n`,(0,n.jsxDEV)(e.li,{children:\"Statistical significance testing for architectural comparisons\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:532,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:\"Qualitative analysis of generated text quality\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:533,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:\"Quantitative metrics (perplexity, BLEU scores where applicable)\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:534,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:\"Memory and computational complexity analysis\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:535,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:532,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.hr,{},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:537,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h2,{children:\"\\u{1F50D} Common Pitfalls & Debugging Guide\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:539,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h3,{children:\"Implementation Issues\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:541,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.p,{children:[(0,n.jsxDEV)(e.strong,{children:\"Problem:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:543,columnNumber:1},this),\" Attention weights don't sum to 1\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:543,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.pre,{children:(0,n.jsxDEV)(e.code,{className:\"language-python\",children:`# Wrong: Softmax over wrong dimension\nattention_weights = F.softmax(scores, dim=-2)  # \\u274C\n\n# Correct: Softmax over key dimension\nattention_weights = F.softmax(scores, dim=-1)  # \\u2705\n`},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:544,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:544,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.p,{children:[(0,n.jsxDEV)(e.strong,{children:\"Problem:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:552,columnNumber:1},this),\" Gradient explosion during training\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:552,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.pre,{children:(0,n.jsxDEV)(e.code,{className:\"language-python\",children:`# Add gradient clipping\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n# Use proper weight initialization\ndef _init_weights(self, module):\n    if isinstance(module, nn.Linear):\n        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n`},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:553,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:553,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.p,{children:[(0,n.jsxDEV)(e.strong,{children:\"Problem:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:563,columnNumber:1},this),\" Slow training speed\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:563,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.pre,{children:(0,n.jsxDEV)(e.code,{className:\"language-python\",children:`# Enable mixed precision training\nscaler = torch.cuda.amp.GradScaler()\n\nwith torch.cuda.amp.autocast():\n    logits = model(input_ids)\n    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n\nscaler.scale(loss).backward()\nscaler.step(optimizer)\nscaler.update()\n`},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:564,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:564,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h3,{children:\"Conceptual Issues\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:577,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.p,{children:[(0,n.jsxDEV)(e.strong,{children:\"Q: Why do we scale attention scores by \\u221Ad_k?\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:579,columnNumber:1},this),`\nA: Without scaling, the dot products grow large as d_k increases, pushing the softmax into regions with extremely small gradients, making training unstable.`]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:579,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.p,{children:[(0,n.jsxDEV)(e.strong,{children:\"Q: Why use sinusoidal positional encodings instead of learned embeddings?\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:582,columnNumber:1},this),`\nA: Sinusoidal encodings allow the model to extrapolate to sequence lengths longer than those seen during training, while learned embeddings are fixed to the maximum training length.`]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:582,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.p,{children:[(0,n.jsxDEV)(e.strong,{children:\"Q: What's the difference between pre-norm and post-norm architectures?\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:585,columnNumber:1},this),`\nA: Pre-norm (used in modern transformers) applies layer normalization before the sub-layers, leading to more stable training and better gradient flow.`]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:585,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.hr,{},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:588,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h2,{children:\"\\u{1F4DA} Extended Learning Resources\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:590,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h3,{children:\"Essential Papers\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:592,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.ol,{children:[`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:'\"Attention Is All You Need\"'},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:593,columnNumber:4},this),\" (Vaswani et al., 2017) - The original transformer paper\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:593,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:'\"Language Models are Unsupervised Multitask Learners\"'},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:594,columnNumber:4},this),\" (Radford et al., 2019) - GPT-2 architecture and scaling insights\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:594,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:'\"On Layer Normalization in the Transformer Architecture\"'},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:595,columnNumber:4},this),\" (Xiong et al., 2020) - Pre-norm vs post-norm analysis\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:595,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:593,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h3,{children:\"Technical Deep Dives\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:597,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.ul,{children:[`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"The Illustrated Transformer\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:598,columnNumber:3},this),\" by Jay Alammar - Visual explanations of transformer mechanics\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:598,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"The Annotated Transformer\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:599,columnNumber:3},this),\" by Harvard NLP - Code walkthrough with mathematical explanations\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:599,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Transformer Circuits Thread\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:600,columnNumber:3},this),\" by Anthropic - Understanding what transformers learn\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:600,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:598,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h3,{children:\"Implementation References\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:602,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.ul,{children:[`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"nanoGPT\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:603,columnNumber:3},this),\" by Andrej Karpathy - Minimal, educational GPT implementation\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:603,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Transformers Library\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:604,columnNumber:3},this),\" by Hugging Face - Production-ready implementations\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:604,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Fairseq\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:605,columnNumber:3},this),\" by Facebook AI - Research-oriented transformer implementations\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:605,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:603,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.hr,{},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:607,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h2,{children:\"\\u{1F3AF} Success Metrics & Next Steps\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:609,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h3,{children:\"Week 2 Completion Criteria\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:611,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.ul,{children:[`\n`,(0,n.jsxDEV)(e.li,{children:\"[ ] All unit tests pass with >90% code coverage\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:612,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:\"[ ] Model achieves target perplexity on validation set\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:613,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:\"[ ] Documentation includes mathematical derivations and architecture diagrams\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:614,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:\"[ ] Generated text demonstrates basic coherence and grammar\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:615,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:\"[ ] Code follows professional standards (formatting, documentation, version control)\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:616,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:612,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h3,{children:\"Preparation for Week 3\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:618,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.ul,{children:[`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Review fine-tuning literature\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:619,columnNumber:3},this),\" - Understand transfer learning for language models\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:619,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Explore parameter-efficient methods\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:620,columnNumber:3},this),\" - LoRA, adapters, and prompt tuning\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:620,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Study evaluation metrics\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:621,columnNumber:3},this),\" - BLEU, ROUGE, perplexity, and human evaluation\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:621,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Familiarize with domain adaptation\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:622,columnNumber:3},this),\" - How to specialize models for specific tasks\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:622,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:619,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.h3,{children:\"Career Development Notes\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:624,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.p,{children:\"This implementation demonstrates several key skills employers look for:\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:625,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.ul,{children:[`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Deep technical knowledge\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:626,columnNumber:3},this),\" - Understanding transformers at the mathematical level\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:626,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Implementation ability\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:627,columnNumber:3},this),\" - Building complex systems from scratch\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:627,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Optimization skills\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:628,columnNumber:3},this),\" - Making models efficient and scalable\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:628,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Documentation\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:629,columnNumber:3},this),\" - Explaining complex concepts clearly\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:629,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.li,{children:[(0,n.jsxDEV)(e.strong,{children:\"Testing\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:630,columnNumber:3},this),\" - Ensuring code reliability and correctness\"]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:630,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:626,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.p,{children:\"Add this project to your portfolio with emphasis on the mathematical depth and implementation quality. The ability to build transformers from scratch sets you apart from developers who only use pre-built models.\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:632,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.hr,{},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:634,columnNumber:1},this),`\n`,(0,n.jsxDEV)(e.p,{children:(0,n.jsxDEV)(e.em,{children:\"Next week, we'll take your transformer implementation and fine-tune it for specific domains using parameter-efficient techniques like LoRA. You'll learn how to adapt pre-trained models for specialized tasks while maintaining computational efficiency.\"},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:636,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:636,columnNumber:1},this)]},void 0,!0,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\",lineNumber:1,columnNumber:1},this)}function be(r={}){let{wrapper:e}=r.components||{};return e?(0,n.jsxDEV)(e,Object.assign({},r,{children:(0,n.jsxDEV)(L,r,void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\"},this)}),void 0,!1,{fileName:\"/Users/danielkliewer/page06/src/app/genai-course/modules/fine-tune/_mdx_bundler_entry_point-9d67e0e3-f1e8-4d23-b9d5-b95287ac293d.mdx\"},this):L(r)}var pe=be;return ce(_e);})();\n/*! Bundled license information:\n\nreact/cjs/react-jsx-dev-runtime.development.js:\n  (**\n   * @license React\n   * react-jsx-dev-runtime.development.js\n   *\n   * Copyright (c) Meta Platforms, Inc. and affiliates.\n   *\n   * This source code is licensed under the MIT license found in the\n   * LICENSE file in the root directory of this source tree.\n   *)\n*/\n;return Component;"
  },
  "_id": "modules/fine-tune/page.mdx",
  "_raw": {
    "sourceFilePath": "modules/fine-tune/page.mdx",
    "sourceFileName": "page.mdx",
    "sourceFileDir": "modules/fine-tune",
    "contentType": "mdx",
    "flattenedPath": "modules/fine-tune/page"
  },
  "type": "ModuleDoc",
  "slug": "fine-tune"
}